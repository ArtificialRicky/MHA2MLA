{
  "config_general": {
    "lighteval_sha": "?",
    "num_fewshot_seeds": 1,
    "override_batch_size": 96,
    "max_samples": null,
    "job_id": "",
    "start_time": 171834.815258131,
    "end_time": 178379.216371255,
    "total_evaluation_time_secondes": "6544.401113123982",
    "model_name": ".._checkpoints_v5_last8_rope_16200_hf",
    "model_sha": "",
    "model_dtype": "torch.bfloat16",
    "model_size": "256.57 MB"
  },
  "results": {
    "custom|arc:challenge|0": {
      "acc": 0.2354948805460751,
      "acc_stderr": 0.012399451855004748,
      "acc_norm": 0.2687713310580205,
      "acc_norm_stderr": 0.012955065963710686
    },
    "custom|arc:easy|0": {
      "acc": 0.5753367003367004,
      "acc_stderr": 0.010142653687480414,
      "acc_norm": 0.5109427609427609,
      "acc_norm_stderr": 0.010257326131172867
    },
    "custom|hellaswag|0": {
      "acc": 0.3287193786098387,
      "acc_stderr": 0.0046878771831644645,
      "acc_norm": 0.3781119298944433,
      "acc_norm_stderr": 0.004839247332606035
    },
    "custom|mmlu_cloze:abstract_algebra|0": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909283,
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.040936018074033256
    },
    "custom|mmlu_cloze:anatomy|0": {
      "acc": 0.3111111111111111,
      "acc_stderr": 0.0399926287661772,
      "acc_norm": 0.35555555555555557,
      "acc_norm_stderr": 0.04135176749720386
    },
    "custom|mmlu_cloze:astronomy|0": {
      "acc": 0.27631578947368424,
      "acc_stderr": 0.03639057569952925,
      "acc_norm": 0.3355263157894737,
      "acc_norm_stderr": 0.038424985593952694
    },
    "custom|mmlu_cloze:business_ethics|0": {
      "acc": 0.43,
      "acc_stderr": 0.04975698519562428,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "custom|mmlu_cloze:clinical_knowledge|0": {
      "acc": 0.25660377358490566,
      "acc_stderr": 0.02688064788905197,
      "acc_norm": 0.33962264150943394,
      "acc_norm_stderr": 0.029146904747798335
    },
    "custom|mmlu_cloze:college_biology|0": {
      "acc": 0.3125,
      "acc_stderr": 0.038760854559127644,
      "acc_norm": 0.2986111111111111,
      "acc_norm_stderr": 0.038270523579507554
    },
    "custom|mmlu_cloze:college_chemistry|0": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768078,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768079
    },
    "custom|mmlu_cloze:college_computer_science|0": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542129,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "custom|mmlu_cloze:college_mathematics|0": {
      "acc": 0.18,
      "acc_stderr": 0.03861229196653694,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.04163331998932269
    },
    "custom|mmlu_cloze:college_medicine|0": {
      "acc": 0.2947976878612717,
      "acc_stderr": 0.03476599607516478,
      "acc_norm": 0.2658959537572254,
      "acc_norm_stderr": 0.033687629322594295
    },
    "custom|mmlu_cloze:college_physics|0": {
      "acc": 0.18627450980392157,
      "acc_stderr": 0.038739587141493524,
      "acc_norm": 0.1568627450980392,
      "acc_norm_stderr": 0.036186648199362445
    },
    "custom|mmlu_cloze:computer_security|0": {
      "acc": 0.29,
      "acc_stderr": 0.04560480215720684,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "custom|mmlu_cloze:conceptual_physics|0": {
      "acc": 0.32340425531914896,
      "acc_stderr": 0.030579442773610337,
      "acc_norm": 0.23829787234042554,
      "acc_norm_stderr": 0.027851252973889757
    },
    "custom|mmlu_cloze:econometrics|0": {
      "acc": 0.24561403508771928,
      "acc_stderr": 0.040493392977481425,
      "acc_norm": 0.21929824561403508,
      "acc_norm_stderr": 0.03892431106518755
    },
    "custom|mmlu_cloze:electrical_engineering|0": {
      "acc": 0.2413793103448276,
      "acc_stderr": 0.03565998174135302,
      "acc_norm": 0.2689655172413793,
      "acc_norm_stderr": 0.036951833116502325
    },
    "custom|mmlu_cloze:elementary_mathematics|0": {
      "acc": 0.2275132275132275,
      "acc_stderr": 0.02159126940782378,
      "acc_norm": 0.23544973544973544,
      "acc_norm_stderr": 0.021851509822031715
    },
    "custom|mmlu_cloze:formal_logic|0": {
      "acc": 0.30158730158730157,
      "acc_stderr": 0.04104947269903394,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.04006168083848877
    },
    "custom|mmlu_cloze:global_facts|0": {
      "acc": 0.26,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "custom|mmlu_cloze:high_school_biology|0": {
      "acc": 0.29354838709677417,
      "acc_stderr": 0.025906087021319288,
      "acc_norm": 0.36451612903225805,
      "acc_norm_stderr": 0.027379871229943238
    },
    "custom|mmlu_cloze:high_school_chemistry|0": {
      "acc": 0.1921182266009852,
      "acc_stderr": 0.02771931570961477,
      "acc_norm": 0.2019704433497537,
      "acc_norm_stderr": 0.028247350122180267
    },
    "custom|mmlu_cloze:high_school_computer_science|0": {
      "acc": 0.21,
      "acc_stderr": 0.040936018074033256,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.046482319871173156
    },
    "custom|mmlu_cloze:high_school_european_history|0": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.03453131801885415,
      "acc_norm": 0.4121212121212121,
      "acc_norm_stderr": 0.03843566993588717
    },
    "custom|mmlu_cloze:high_school_geography|0": {
      "acc": 0.30303030303030304,
      "acc_stderr": 0.03274287914026867,
      "acc_norm": 0.31313131313131315,
      "acc_norm_stderr": 0.033042050878136525
    },
    "custom|mmlu_cloze:high_school_government_and_politics|0": {
      "acc": 0.3316062176165803,
      "acc_stderr": 0.03397636541089117,
      "acc_norm": 0.38341968911917096,
      "acc_norm_stderr": 0.03508984236295343
    },
    "custom|mmlu_cloze:high_school_macroeconomics|0": {
      "acc": 0.27692307692307694,
      "acc_stderr": 0.022688042352424997,
      "acc_norm": 0.258974358974359,
      "acc_norm_stderr": 0.022211106810061665
    },
    "custom|mmlu_cloze:high_school_mathematics|0": {
      "acc": 0.12962962962962962,
      "acc_stderr": 0.02047991025332072,
      "acc_norm": 0.14814814814814814,
      "acc_norm_stderr": 0.02165977842211803
    },
    "custom|mmlu_cloze:high_school_microeconomics|0": {
      "acc": 0.25630252100840334,
      "acc_stderr": 0.028359620870533953,
      "acc_norm": 0.37815126050420167,
      "acc_norm_stderr": 0.031499305777849054
    },
    "custom|mmlu_cloze:high_school_physics|0": {
      "acc": 0.2847682119205298,
      "acc_stderr": 0.03684881521389023,
      "acc_norm": 0.2847682119205298,
      "acc_norm_stderr": 0.036848815213890225
    },
    "custom|mmlu_cloze:high_school_psychology|0": {
      "acc": 0.3981651376146789,
      "acc_stderr": 0.020987989422654257,
      "acc_norm": 0.3541284403669725,
      "acc_norm_stderr": 0.020504729013829104
    },
    "custom|mmlu_cloze:high_school_statistics|0": {
      "acc": 0.2824074074074074,
      "acc_stderr": 0.030701372111510934,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.031674687068289784
    },
    "custom|mmlu_cloze:high_school_us_history|0": {
      "acc": 0.27941176470588236,
      "acc_stderr": 0.03149328104507957,
      "acc_norm": 0.3284313725490196,
      "acc_norm_stderr": 0.03296245110172227
    },
    "custom|mmlu_cloze:high_school_world_history|0": {
      "acc": 0.28270042194092826,
      "acc_stderr": 0.029312814153955917,
      "acc_norm": 0.2911392405063291,
      "acc_norm_stderr": 0.029571601065753374
    },
    "custom|mmlu_cloze:human_aging|0": {
      "acc": 0.3632286995515695,
      "acc_stderr": 0.032277904428505,
      "acc_norm": 0.3094170403587444,
      "acc_norm_stderr": 0.031024411740572206
    },
    "custom|mmlu_cloze:human_sexuality|0": {
      "acc": 0.37404580152671757,
      "acc_stderr": 0.042438692422305246,
      "acc_norm": 0.29770992366412213,
      "acc_norm_stderr": 0.04010358942462203
    },
    "custom|mmlu_cloze:international_law|0": {
      "acc": 0.1487603305785124,
      "acc_stderr": 0.032484700838071943,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.04065578140908705
    },
    "custom|mmlu_cloze:jurisprudence|0": {
      "acc": 0.19444444444444445,
      "acc_stderr": 0.038260763248848646,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04557239513497752
    },
    "custom|mmlu_cloze:logical_fallacies|0": {
      "acc": 0.24539877300613497,
      "acc_stderr": 0.03380939813943354,
      "acc_norm": 0.31901840490797545,
      "acc_norm_stderr": 0.03661997551073836
    },
    "custom|mmlu_cloze:machine_learning|0": {
      "acc": 0.25892857142857145,
      "acc_stderr": 0.041577515398656284,
      "acc_norm": 0.30357142857142855,
      "acc_norm_stderr": 0.04364226155841044
    },
    "custom|mmlu_cloze:management|0": {
      "acc": 0.27184466019417475,
      "acc_stderr": 0.044052680241409216,
      "acc_norm": 0.3883495145631068,
      "acc_norm_stderr": 0.0482572933735639
    },
    "custom|mmlu_cloze:marketing|0": {
      "acc": 0.38461538461538464,
      "acc_stderr": 0.03187195347942466,
      "acc_norm": 0.3888888888888889,
      "acc_norm_stderr": 0.03193705726200293
    },
    "custom|mmlu_cloze:medical_genetics|0": {
      "acc": 0.29,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "custom|mmlu_cloze:miscellaneous|0": {
      "acc": 0.36270753512132825,
      "acc_stderr": 0.017192708674602302,
      "acc_norm": 0.34355044699872284,
      "acc_norm_stderr": 0.016982145632652466
    },
    "custom|mmlu_cloze:moral_disputes|0": {
      "acc": 0.2398843930635838,
      "acc_stderr": 0.02298959254312357,
      "acc_norm": 0.22832369942196531,
      "acc_norm_stderr": 0.022598703804321617
    },
    "custom|mmlu_cloze:moral_scenarios|0": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.26927374301675977,
      "acc_norm_stderr": 0.01483561658288258
    },
    "custom|mmlu_cloze:nutrition|0": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.025646863097137908,
      "acc_norm": 0.3202614379084967,
      "acc_norm_stderr": 0.026716118380156844
    },
    "custom|mmlu_cloze:philosophy|0": {
      "acc": 0.26366559485530544,
      "acc_stderr": 0.02502553850053234,
      "acc_norm": 0.2797427652733119,
      "acc_norm_stderr": 0.025494259350694902
    },
    "custom|mmlu_cloze:prehistory|0": {
      "acc": 0.345679012345679,
      "acc_stderr": 0.026462487777001872,
      "acc_norm": 0.24382716049382716,
      "acc_norm_stderr": 0.023891879541959597
    },
    "custom|mmlu_cloze:professional_accounting|0": {
      "acc": 0.2624113475177305,
      "acc_stderr": 0.026244920349842996,
      "acc_norm": 0.22695035460992907,
      "acc_norm_stderr": 0.024987106365642966
    },
    "custom|mmlu_cloze:professional_law|0": {
      "acc": 0.2503259452411995,
      "acc_stderr": 0.011064151027165445,
      "acc_norm": 0.27183833116036504,
      "acc_norm_stderr": 0.011363135278651411
    },
    "custom|mmlu_cloze:professional_medicine|0": {
      "acc": 0.2426470588235294,
      "acc_stderr": 0.026040662474201264,
      "acc_norm": 0.30514705882352944,
      "acc_norm_stderr": 0.027971541370170595
    },
    "custom|mmlu_cloze:professional_psychology|0": {
      "acc": 0.2826797385620915,
      "acc_stderr": 0.018217269552053442,
      "acc_norm": 0.2761437908496732,
      "acc_norm_stderr": 0.018087276935663137
    },
    "custom|mmlu_cloze:public_relations|0": {
      "acc": 0.37272727272727274,
      "acc_stderr": 0.04631381319425464,
      "acc_norm": 0.2545454545454545,
      "acc_norm_stderr": 0.041723430387053825
    },
    "custom|mmlu_cloze:security_studies|0": {
      "acc": 0.3183673469387755,
      "acc_stderr": 0.029822533793982052,
      "acc_norm": 0.24081632653061225,
      "acc_norm_stderr": 0.027372942201788167
    },
    "custom|mmlu_cloze:sociology|0": {
      "acc": 0.2736318407960199,
      "acc_stderr": 0.03152439186555404,
      "acc_norm": 0.2935323383084577,
      "acc_norm_stderr": 0.03220024104534204
    },
    "custom|mmlu_cloze:us_foreign_policy|0": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.0446196043338474
    },
    "custom|mmlu_cloze:virology|0": {
      "acc": 0.25903614457831325,
      "acc_stderr": 0.03410646614071856,
      "acc_norm": 0.3132530120481928,
      "acc_norm_stderr": 0.03610805018031023
    },
    "custom|mmlu_cloze:world_religions|0": {
      "acc": 0.3391812865497076,
      "acc_stderr": 0.036310534964889056,
      "acc_norm": 0.39766081871345027,
      "acc_norm_stderr": 0.0375363895576169
    },
    "custom|openbookqa|0": {
      "acc": 0.198,
      "acc_stderr": 0.017838958963847233,
      "acc_norm": 0.318,
      "acc_norm_stderr": 0.020847571620814007
    },
    "custom|piqa|0": {
      "acc": 0.6643090315560392,
      "acc_stderr": 0.011017938116656308,
      "acc_norm": 0.6702937976060935,
      "acc_norm_stderr": 0.010968357083095152
    },
    "custom|winogrande|0": {
      "acc": 0.5090765588003157,
      "acc_stderr": 0.014050170094497704,
      "acc_norm": 0.4956590370955012,
      "acc_norm_stderr": 0.0140519560640769
    },
    "custom|trivia_qa|0": {
      "qem": 0.005907267053053946,
      "qem_stderr": 0.000572083238174626
    },
    "custom|arc:_average|0": {
      "acc": 0.40541579044138776,
      "acc_stderr": 0.01127105277124258,
      "acc_norm": 0.38985704600039073,
      "acc_norm_stderr": 0.011606196047441775
    },
    "custom|mmlu_cloze:_average|0": {
      "acc": 0.2777957326475164,
      "acc_stderr": 0.0331575910982594,
      "acc_norm": 0.2947975552894719,
      "acc_norm_stderr": 0.03385742058726162
    },
    "all": {
      "acc": 0.2911951319167842,
      "acc_stderr": 0.031113011785737096,
      "acc_norm": 0.3086545953666146,
      "acc_norm_stderr": 0.03180623012173632,
      "qem": 0.005907267053053946,
      "qem_stderr": 0.000572083238174626
    }
  },
  "versions": {
    "custom|arc:challenge|0": 0,
    "custom|arc:easy|0": 0,
    "custom|hellaswag|0": 0,
    "custom|mmlu_cloze:abstract_algebra|0": 0,
    "custom|mmlu_cloze:anatomy|0": 0,
    "custom|mmlu_cloze:astronomy|0": 0,
    "custom|mmlu_cloze:business_ethics|0": 0,
    "custom|mmlu_cloze:clinical_knowledge|0": 0,
    "custom|mmlu_cloze:college_biology|0": 0,
    "custom|mmlu_cloze:college_chemistry|0": 0,
    "custom|mmlu_cloze:college_computer_science|0": 0,
    "custom|mmlu_cloze:college_mathematics|0": 0,
    "custom|mmlu_cloze:college_medicine|0": 0,
    "custom|mmlu_cloze:college_physics|0": 0,
    "custom|mmlu_cloze:computer_security|0": 0,
    "custom|mmlu_cloze:conceptual_physics|0": 0,
    "custom|mmlu_cloze:econometrics|0": 0,
    "custom|mmlu_cloze:electrical_engineering|0": 0,
    "custom|mmlu_cloze:elementary_mathematics|0": 0,
    "custom|mmlu_cloze:formal_logic|0": 0,
    "custom|mmlu_cloze:global_facts|0": 0,
    "custom|mmlu_cloze:high_school_biology|0": 0,
    "custom|mmlu_cloze:high_school_chemistry|0": 0,
    "custom|mmlu_cloze:high_school_computer_science|0": 0,
    "custom|mmlu_cloze:high_school_european_history|0": 0,
    "custom|mmlu_cloze:high_school_geography|0": 0,
    "custom|mmlu_cloze:high_school_government_and_politics|0": 0,
    "custom|mmlu_cloze:high_school_macroeconomics|0": 0,
    "custom|mmlu_cloze:high_school_mathematics|0": 0,
    "custom|mmlu_cloze:high_school_microeconomics|0": 0,
    "custom|mmlu_cloze:high_school_physics|0": 0,
    "custom|mmlu_cloze:high_school_psychology|0": 0,
    "custom|mmlu_cloze:high_school_statistics|0": 0,
    "custom|mmlu_cloze:high_school_us_history|0": 0,
    "custom|mmlu_cloze:high_school_world_history|0": 0,
    "custom|mmlu_cloze:human_aging|0": 0,
    "custom|mmlu_cloze:human_sexuality|0": 0,
    "custom|mmlu_cloze:international_law|0": 0,
    "custom|mmlu_cloze:jurisprudence|0": 0,
    "custom|mmlu_cloze:logical_fallacies|0": 0,
    "custom|mmlu_cloze:machine_learning|0": 0,
    "custom|mmlu_cloze:management|0": 0,
    "custom|mmlu_cloze:marketing|0": 0,
    "custom|mmlu_cloze:medical_genetics|0": 0,
    "custom|mmlu_cloze:miscellaneous|0": 0,
    "custom|mmlu_cloze:moral_disputes|0": 0,
    "custom|mmlu_cloze:moral_scenarios|0": 0,
    "custom|mmlu_cloze:nutrition|0": 0,
    "custom|mmlu_cloze:philosophy|0": 0,
    "custom|mmlu_cloze:prehistory|0": 0,
    "custom|mmlu_cloze:professional_accounting|0": 0,
    "custom|mmlu_cloze:professional_law|0": 0,
    "custom|mmlu_cloze:professional_medicine|0": 0,
    "custom|mmlu_cloze:professional_psychology|0": 0,
    "custom|mmlu_cloze:public_relations|0": 0,
    "custom|mmlu_cloze:security_studies|0": 0,
    "custom|mmlu_cloze:sociology|0": 0,
    "custom|mmlu_cloze:us_foreign_policy|0": 0,
    "custom|mmlu_cloze:virology|0": 0,
    "custom|mmlu_cloze:world_religions|0": 0,
    "custom|openbookqa|0": 0,
    "custom|piqa|0": 0,
    "custom|trivia_qa|0": 0,
    "custom|winogrande|0": 0
  },
  "config_tasks": {
    "custom|arc:challenge": {
      "name": "arc:challenge",
      "prompt_function": "arc",
      "hf_repo": "ai2_arc",
      "hf_subset": "ARC-Challenge",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1172,
      "effective_num_docs": 1172,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|arc:easy": {
      "name": "arc:easy",
      "prompt_function": "arc",
      "hf_repo": "ai2_arc",
      "hf_subset": "ARC-Easy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 2376,
      "effective_num_docs": 2376,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|hellaswag": {
      "name": "hellaswag",
      "prompt_function": "hellaswag_prompt",
      "hf_repo": "hellaswag",
      "hf_subset": "default",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 10042,
      "effective_num_docs": 10042,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:abstract_algebra": {
      "name": "mmlu_cloze:abstract_algebra",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "abstract_algebra",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:anatomy": {
      "name": "mmlu_cloze:anatomy",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "anatomy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:astronomy": {
      "name": "mmlu_cloze:astronomy",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "astronomy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:business_ethics": {
      "name": "mmlu_cloze:business_ethics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "business_ethics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:clinical_knowledge": {
      "name": "mmlu_cloze:clinical_knowledge",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "clinical_knowledge",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_biology": {
      "name": "mmlu_cloze:college_biology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_biology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_chemistry": {
      "name": "mmlu_cloze:college_chemistry",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_chemistry",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_computer_science": {
      "name": "mmlu_cloze:college_computer_science",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_computer_science",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_mathematics": {
      "name": "mmlu_cloze:college_mathematics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_mathematics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_medicine": {
      "name": "mmlu_cloze:college_medicine",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_medicine",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_physics": {
      "name": "mmlu_cloze:college_physics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_physics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:computer_security": {
      "name": "mmlu_cloze:computer_security",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "computer_security",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:conceptual_physics": {
      "name": "mmlu_cloze:conceptual_physics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "conceptual_physics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:econometrics": {
      "name": "mmlu_cloze:econometrics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "econometrics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:electrical_engineering": {
      "name": "mmlu_cloze:electrical_engineering",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "electrical_engineering",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:elementary_mathematics": {
      "name": "mmlu_cloze:elementary_mathematics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "elementary_mathematics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:formal_logic": {
      "name": "mmlu_cloze:formal_logic",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "formal_logic",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:global_facts": {
      "name": "mmlu_cloze:global_facts",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "global_facts",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_biology": {
      "name": "mmlu_cloze:high_school_biology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_biology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_chemistry": {
      "name": "mmlu_cloze:high_school_chemistry",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_chemistry",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_computer_science": {
      "name": "mmlu_cloze:high_school_computer_science",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_computer_science",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_european_history": {
      "name": "mmlu_cloze:high_school_european_history",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_european_history",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_geography": {
      "name": "mmlu_cloze:high_school_geography",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_geography",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_government_and_politics": {
      "name": "mmlu_cloze:high_school_government_and_politics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_macroeconomics": {
      "name": "mmlu_cloze:high_school_macroeconomics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_mathematics": {
      "name": "mmlu_cloze:high_school_mathematics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_mathematics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_microeconomics": {
      "name": "mmlu_cloze:high_school_microeconomics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_physics": {
      "name": "mmlu_cloze:high_school_physics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_physics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_psychology": {
      "name": "mmlu_cloze:high_school_psychology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_psychology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_statistics": {
      "name": "mmlu_cloze:high_school_statistics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_statistics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_us_history": {
      "name": "mmlu_cloze:high_school_us_history",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_us_history",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_world_history": {
      "name": "mmlu_cloze:high_school_world_history",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_world_history",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:human_aging": {
      "name": "mmlu_cloze:human_aging",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_aging",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:human_sexuality": {
      "name": "mmlu_cloze:human_sexuality",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_sexuality",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:international_law": {
      "name": "mmlu_cloze:international_law",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "international_law",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:jurisprudence": {
      "name": "mmlu_cloze:jurisprudence",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "jurisprudence",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:logical_fallacies": {
      "name": "mmlu_cloze:logical_fallacies",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "logical_fallacies",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:machine_learning": {
      "name": "mmlu_cloze:machine_learning",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "machine_learning",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:management": {
      "name": "mmlu_cloze:management",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "management",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:marketing": {
      "name": "mmlu_cloze:marketing",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "marketing",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:medical_genetics": {
      "name": "mmlu_cloze:medical_genetics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "medical_genetics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:miscellaneous": {
      "name": "mmlu_cloze:miscellaneous",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "miscellaneous",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:moral_disputes": {
      "name": "mmlu_cloze:moral_disputes",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_disputes",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:moral_scenarios": {
      "name": "mmlu_cloze:moral_scenarios",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_scenarios",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:nutrition": {
      "name": "mmlu_cloze:nutrition",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "nutrition",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:philosophy": {
      "name": "mmlu_cloze:philosophy",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "philosophy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:prehistory": {
      "name": "mmlu_cloze:prehistory",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "prehistory",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:professional_accounting": {
      "name": "mmlu_cloze:professional_accounting",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_accounting",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:professional_law": {
      "name": "mmlu_cloze:professional_law",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_law",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:professional_medicine": {
      "name": "mmlu_cloze:professional_medicine",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_medicine",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:professional_psychology": {
      "name": "mmlu_cloze:professional_psychology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_psychology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:public_relations": {
      "name": "mmlu_cloze:public_relations",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "public_relations",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:security_studies": {
      "name": "mmlu_cloze:security_studies",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "security_studies",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:sociology": {
      "name": "mmlu_cloze:sociology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "sociology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:us_foreign_policy": {
      "name": "mmlu_cloze:us_foreign_policy",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "us_foreign_policy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:virology": {
      "name": "mmlu_cloze:virology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "virology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:world_religions": {
      "name": "mmlu_cloze:world_religions",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "world_religions",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|openbookqa": {
      "name": "openbookqa",
      "prompt_function": "openbookqa",
      "hf_repo": "openbookqa",
      "hf_subset": "main",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 500,
      "effective_num_docs": 500,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|piqa": {
      "name": "piqa",
      "prompt_function": "piqa_harness",
      "hf_repo": "piqa",
      "hf_subset": "plain_text",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1838,
      "effective_num_docs": 1838,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|trivia_qa": {
      "name": "trivia_qa",
      "prompt_function": "triviaqa",
      "hf_repo": "mandarjoshi/trivia_qa",
      "hf_subset": "rc.nocontext",
      "metric": [
        {
          "metric_name": "qem",
          "higher_is_better": true,
          "category": "3",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation"
      ],
      "trust_dataset": true,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling_from_train",
      "generation_size": 20,
      "generation_grammar": null,
      "stop_sequence": [
        "\n",
        ".",
        ","
      ],
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 17944,
      "effective_num_docs": 17944,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|winogrande": {
      "name": "winogrande",
      "prompt_function": "winogrande",
      "hf_repo": "winogrande",
      "hf_subset": "winogrande_xl",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1267,
      "effective_num_docs": 1267,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    }
  },
  "summary_tasks": {
    "custom|arc:challenge|0": {
      "hashes": {
        "hash_examples": "17b0cae357c0259e",
        "hash_full_prompts": "17b0cae357c0259e",
        "hash_input_tokens": "ad17dbd81a2a3d26",
        "hash_cont_tokens": "c512920b4065b2e5"
      },
      "truncated": 0,
      "non_truncated": 1172,
      "padded": 4687,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|arc:easy|0": {
      "hashes": {
        "hash_examples": "63703c3cdff55bec",
        "hash_full_prompts": "63703c3cdff55bec",
        "hash_input_tokens": "40eb9e5845d99163",
        "hash_cont_tokens": "9819b2bd4e06b652"
      },
      "truncated": 0,
      "non_truncated": 2376,
      "padded": 9501,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|hellaswag|0": {
      "hashes": {
        "hash_examples": "31985c805c3a737e",
        "hash_full_prompts": "31985c805c3a737e",
        "hash_input_tokens": "6ed1e652f6c51d42",
        "hash_cont_tokens": "228934e77282dbf3"
      },
      "truncated": 0,
      "non_truncated": 10042,
      "padded": 39678,
      "non_padded": 490,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:abstract_algebra|0": {
      "hashes": {
        "hash_examples": "ff00c12a680621ba",
        "hash_full_prompts": "ff00c12a680621ba",
        "hash_input_tokens": "3deb93f0a8b89a12",
        "hash_cont_tokens": "3b1fd217ca5eae7d"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 394,
      "non_padded": 6,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:anatomy|0": {
      "hashes": {
        "hash_examples": "7f9c9593991d6727",
        "hash_full_prompts": "7f9c9593991d6727",
        "hash_input_tokens": "83eb2604b438a0ac",
        "hash_cont_tokens": "da5637ac437db1cd"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 506,
      "non_padded": 34,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:astronomy|0": {
      "hashes": {
        "hash_examples": "ff5985a306787836",
        "hash_full_prompts": "ff5985a306787836",
        "hash_input_tokens": "665132a1dc5de70c",
        "hash_cont_tokens": "5c634041250c2777"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 593,
      "non_padded": 15,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:business_ethics|0": {
      "hashes": {
        "hash_examples": "e3fe02a23d08c2d0",
        "hash_full_prompts": "e3fe02a23d08c2d0",
        "hash_input_tokens": "8b4c3ffaa76df3bf",
        "hash_cont_tokens": "53c23c096d6fbbc5"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 389,
      "non_padded": 11,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:clinical_knowledge|0": {
      "hashes": {
        "hash_examples": "9b962be8e1615cd5",
        "hash_full_prompts": "9b962be8e1615cd5",
        "hash_input_tokens": "e23904e043567e98",
        "hash_cont_tokens": "f0cbafe3545dd281"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 989,
      "non_padded": 71,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_biology|0": {
      "hashes": {
        "hash_examples": "17b4ca841de3a2a3",
        "hash_full_prompts": "17b4ca841de3a2a3",
        "hash_input_tokens": "ceb01e0604c75ada",
        "hash_cont_tokens": "bd120945b8cd6191"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 559,
      "non_padded": 17,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_chemistry|0": {
      "hashes": {
        "hash_examples": "babea49005fd8249",
        "hash_full_prompts": "babea49005fd8249",
        "hash_input_tokens": "df472866ea2a8a3d",
        "hash_cont_tokens": "01560744da3bb1ae"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 380,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_computer_science|0": {
      "hashes": {
        "hash_examples": "47f4fef1846c8914",
        "hash_full_prompts": "47f4fef1846c8914",
        "hash_input_tokens": "485962986df7fbb7",
        "hash_cont_tokens": "0e6deead73387308"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 386,
      "non_padded": 14,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_mathematics|0": {
      "hashes": {
        "hash_examples": "4d3686d599963414",
        "hash_full_prompts": "4d3686d599963414",
        "hash_input_tokens": "2e62aed19b952d56",
        "hash_cont_tokens": "8dafab7e4fbb7fbe"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 391,
      "non_padded": 9,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_medicine|0": {
      "hashes": {
        "hash_examples": "7209619ecac6f235",
        "hash_full_prompts": "7209619ecac6f235",
        "hash_input_tokens": "537efb793ce96ddc",
        "hash_cont_tokens": "84342b2b76c98a6e"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 669,
      "non_padded": 23,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_physics|0": {
      "hashes": {
        "hash_examples": "6131b6c60dd7f055",
        "hash_full_prompts": "6131b6c60dd7f055",
        "hash_input_tokens": "43de489cb2fb254d",
        "hash_cont_tokens": "9d0e49ba8dc5989a"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 399,
      "non_padded": 9,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:computer_security|0": {
      "hashes": {
        "hash_examples": "99539c9a5bc98a59",
        "hash_full_prompts": "99539c9a5bc98a59",
        "hash_input_tokens": "9e4ddd2397d35aa3",
        "hash_cont_tokens": "01121a08afaaf8dc"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 382,
      "non_padded": 18,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:conceptual_physics|0": {
      "hashes": {
        "hash_examples": "4e15015839d00858",
        "hash_full_prompts": "4e15015839d00858",
        "hash_input_tokens": "bf0f2e7060a7147a",
        "hash_cont_tokens": "e513df10450acb97"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 891,
      "non_padded": 49,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:econometrics|0": {
      "hashes": {
        "hash_examples": "fba5c647465e89e0",
        "hash_full_prompts": "fba5c647465e89e0",
        "hash_input_tokens": "0172a75cff9a807b",
        "hash_cont_tokens": "134b345bdee1a0cb"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 451,
      "non_padded": 5,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:electrical_engineering|0": {
      "hashes": {
        "hash_examples": "2db2ba0cb98cda51",
        "hash_full_prompts": "2db2ba0cb98cda51",
        "hash_input_tokens": "cb93841652392319",
        "hash_cont_tokens": "a683d95cb9fbf0d6"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 556,
      "non_padded": 24,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:elementary_mathematics|0": {
      "hashes": {
        "hash_examples": "f231cd5ae05742bb",
        "hash_full_prompts": "f231cd5ae05742bb",
        "hash_input_tokens": "2c622fca8451ea44",
        "hash_cont_tokens": "094ff4c560f0db79"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1448,
      "non_padded": 64,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:formal_logic|0": {
      "hashes": {
        "hash_examples": "b69d9607d5da536e",
        "hash_full_prompts": "b69d9607d5da536e",
        "hash_input_tokens": "3f9f9808ee4d5c08",
        "hash_cont_tokens": "29db55d952acb85d"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 499,
      "non_padded": 5,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:global_facts|0": {
      "hashes": {
        "hash_examples": "c9f53772e672f6bc",
        "hash_full_prompts": "c9f53772e672f6bc",
        "hash_input_tokens": "ce153de1bd9d7d1d",
        "hash_cont_tokens": "867dcd15c271e15c"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 394,
      "non_padded": 6,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_biology|0": {
      "hashes": {
        "hash_examples": "455027cf6cdd02bc",
        "hash_full_prompts": "455027cf6cdd02bc",
        "hash_input_tokens": "8ac752c32df9f4f5",
        "hash_cont_tokens": "cb3e6a68f47a8eeb"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1208,
      "non_padded": 32,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_chemistry|0": {
      "hashes": {
        "hash_examples": "95d9caac9edbc34d",
        "hash_full_prompts": "95d9caac9edbc34d",
        "hash_input_tokens": "df11c251b9b0d163",
        "hash_cont_tokens": "896d06c20dcec9ec"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 790,
      "non_padded": 22,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_computer_science|0": {
      "hashes": {
        "hash_examples": "6e44706db3791e51",
        "hash_full_prompts": "6e44706db3791e51",
        "hash_input_tokens": "bdeea952887d8dd7",
        "hash_cont_tokens": "9da53a8484fa853e"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 394,
      "non_padded": 6,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_european_history|0": {
      "hashes": {
        "hash_examples": "9078fce41897117d",
        "hash_full_prompts": "9078fce41897117d",
        "hash_input_tokens": "9ad11e4de3ce0c67",
        "hash_cont_tokens": "62e27833dc0027fe"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 660,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_geography|0": {
      "hashes": {
        "hash_examples": "c68adcc34130a2e8",
        "hash_full_prompts": "c68adcc34130a2e8",
        "hash_input_tokens": "4c9748f8e241a30d",
        "hash_cont_tokens": "0dde98155d0c26e6"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 752,
      "non_padded": 40,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_government_and_politics|0": {
      "hashes": {
        "hash_examples": "6f839b19e49a0858",
        "hash_full_prompts": "6f839b19e49a0858",
        "hash_input_tokens": "da410e087149f6bc",
        "hash_cont_tokens": "d74a61eae0b6b657"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 758,
      "non_padded": 14,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_macroeconomics|0": {
      "hashes": {
        "hash_examples": "708a5c05e7adb220",
        "hash_full_prompts": "708a5c05e7adb220",
        "hash_input_tokens": "70302329195e342b",
        "hash_cont_tokens": "6441e265e8f4af19"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1477,
      "non_padded": 83,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_mathematics|0": {
      "hashes": {
        "hash_examples": "47c621dd61fd7790",
        "hash_full_prompts": "47c621dd61fd7790",
        "hash_input_tokens": "c87690d64f3747c0",
        "hash_cont_tokens": "089e3cb73a2e8d5a"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1057,
      "non_padded": 23,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_microeconomics|0": {
      "hashes": {
        "hash_examples": "3c467180d90f6371",
        "hash_full_prompts": "3c467180d90f6371",
        "hash_input_tokens": "19e3ca96d87edb6b",
        "hash_cont_tokens": "b796174aba15bfad"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 895,
      "non_padded": 57,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_physics|0": {
      "hashes": {
        "hash_examples": "89a598cdde43be79",
        "hash_full_prompts": "89a598cdde43be79",
        "hash_input_tokens": "639b1ef057172e8c",
        "hash_cont_tokens": "842e2a4539ef99ab"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 597,
      "non_padded": 7,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_psychology|0": {
      "hashes": {
        "hash_examples": "d8a7bf3f17ec12d0",
        "hash_full_prompts": "d8a7bf3f17ec12d0",
        "hash_input_tokens": "80a9a600eb9eebaa",
        "hash_cont_tokens": "a77f669bd77c6c4d"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2100,
      "non_padded": 80,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_statistics|0": {
      "hashes": {
        "hash_examples": "995374a6caaa97d6",
        "hash_full_prompts": "995374a6caaa97d6",
        "hash_input_tokens": "2beb42f49ef9dd1b",
        "hash_cont_tokens": "7d7c20e92bc841a9"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 845,
      "non_padded": 19,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_us_history|0": {
      "hashes": {
        "hash_examples": "7893e9d07e34cb37",
        "hash_full_prompts": "7893e9d07e34cb37",
        "hash_input_tokens": "83a6ca8cc3ca2519",
        "hash_cont_tokens": "e117d838a8f12367"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 816,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_world_history|0": {
      "hashes": {
        "hash_examples": "48879684e37d1716",
        "hash_full_prompts": "48879684e37d1716",
        "hash_input_tokens": "48c38505c96e1601",
        "hash_cont_tokens": "5ba8b8b1bd31492f"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:human_aging|0": {
      "hashes": {
        "hash_examples": "afae8c53bd6e5f44",
        "hash_full_prompts": "afae8c53bd6e5f44",
        "hash_input_tokens": "b622d2ae2a9a2389",
        "hash_cont_tokens": "3ed6eb737b8d673d"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 843,
      "non_padded": 49,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:human_sexuality|0": {
      "hashes": {
        "hash_examples": "9701f02004912a7a",
        "hash_full_prompts": "9701f02004912a7a",
        "hash_input_tokens": "0590bed70f247fc8",
        "hash_cont_tokens": "a0ca80e5fea94af0"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 504,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:international_law|0": {
      "hashes": {
        "hash_examples": "47955196de2d2c7a",
        "hash_full_prompts": "47955196de2d2c7a",
        "hash_input_tokens": "0835116ea29b754c",
        "hash_cont_tokens": "e294ba24e42cee98"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 463,
      "non_padded": 21,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:jurisprudence|0": {
      "hashes": {
        "hash_examples": "a992eac2b8ae8bc4",
        "hash_full_prompts": "a992eac2b8ae8bc4",
        "hash_input_tokens": "1030710a6d14196a",
        "hash_cont_tokens": "0609d6898d9b68ad"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 419,
      "non_padded": 13,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:logical_fallacies|0": {
      "hashes": {
        "hash_examples": "b0d31ed08f699e6c",
        "hash_full_prompts": "b0d31ed08f699e6c",
        "hash_input_tokens": "d25d432910b49efe",
        "hash_cont_tokens": "185350eb12cb00c8"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 638,
      "non_padded": 14,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:machine_learning|0": {
      "hashes": {
        "hash_examples": "dccdef2bae4461a6",
        "hash_full_prompts": "dccdef2bae4461a6",
        "hash_input_tokens": "07a5e2893c11a61b",
        "hash_cont_tokens": "4246808b9997c875"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 441,
      "non_padded": 7,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:management|0": {
      "hashes": {
        "hash_examples": "f600be25303e1fe2",
        "hash_full_prompts": "f600be25303e1fe2",
        "hash_input_tokens": "10ff2ab1debb80e2",
        "hash_cont_tokens": "14f65210b4927437"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 392,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:marketing|0": {
      "hashes": {
        "hash_examples": "1a0df3ae5e306669",
        "hash_full_prompts": "1a0df3ae5e306669",
        "hash_input_tokens": "dd8fb6f3fc9075b9",
        "hash_cont_tokens": "52968c44575b2158"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 909,
      "non_padded": 27,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:medical_genetics|0": {
      "hashes": {
        "hash_examples": "eb87c9cfd9b7c760",
        "hash_full_prompts": "eb87c9cfd9b7c760",
        "hash_input_tokens": "e34c7451d5600add",
        "hash_cont_tokens": "11ee1d1bad20ade8"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 375,
      "non_padded": 25,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:miscellaneous|0": {
      "hashes": {
        "hash_examples": "f88d724036ba03b7",
        "hash_full_prompts": "f88d724036ba03b7",
        "hash_input_tokens": "2c64d670544bcb79",
        "hash_cont_tokens": "d84457206b083219"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 2994,
      "non_padded": 138,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:moral_disputes|0": {
      "hashes": {
        "hash_examples": "79782c0823005e7b",
        "hash_full_prompts": "79782c0823005e7b",
        "hash_input_tokens": "1ba722114b2f8fa8",
        "hash_cont_tokens": "85c70be841230602"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1341,
      "non_padded": 43,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:moral_scenarios|0": {
      "hashes": {
        "hash_examples": "fe57fdd86442b483",
        "hash_full_prompts": "fe57fdd86442b483",
        "hash_input_tokens": "d24c1c4d9202eaff",
        "hash_cont_tokens": "afb7f4b4b6fba335"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3376,
      "non_padded": 204,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:nutrition|0": {
      "hashes": {
        "hash_examples": "421f206f5957e90f",
        "hash_full_prompts": "421f206f5957e90f",
        "hash_input_tokens": "7231de6e2da3e3a5",
        "hash_cont_tokens": "09243c7215a5c8ff"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1151,
      "non_padded": 73,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:philosophy|0": {
      "hashes": {
        "hash_examples": "c93073e383957fc4",
        "hash_full_prompts": "c93073e383957fc4",
        "hash_input_tokens": "9ccba858995c9a71",
        "hash_cont_tokens": "0391eac7daac6e51"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1161,
      "non_padded": 83,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:prehistory|0": {
      "hashes": {
        "hash_examples": "58ec03e20eae9f90",
        "hash_full_prompts": "58ec03e20eae9f90",
        "hash_input_tokens": "2124f665ee7adbcd",
        "hash_cont_tokens": "f8d95d6e63c30de1"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1246,
      "non_padded": 50,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:professional_accounting|0": {
      "hashes": {
        "hash_examples": "13f7a6023a118512",
        "hash_full_prompts": "13f7a6023a118512",
        "hash_input_tokens": "fc4166484adb1c07",
        "hash_cont_tokens": "47a3d8a4fa184090"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1118,
      "non_padded": 10,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:professional_law|0": {
      "hashes": {
        "hash_examples": "8086d24f4d4e82f4",
        "hash_full_prompts": "8086d24f4d4e82f4",
        "hash_input_tokens": "39cad63797669b26",
        "hash_cont_tokens": "477ac5f3a2582801"
      },
      "truncated": 0,
      "non_truncated": 1534,
      "padded": 6119,
      "non_padded": 17,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:professional_medicine|0": {
      "hashes": {
        "hash_examples": "f0b30b4e786eaeea",
        "hash_full_prompts": "f0b30b4e786eaeea",
        "hash_input_tokens": "709cda008dac9ac1",
        "hash_cont_tokens": "9d55d18fcd916be3"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1084,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:professional_psychology|0": {
      "hashes": {
        "hash_examples": "29fd2b4a194c28ea",
        "hash_full_prompts": "29fd2b4a194c28ea",
        "hash_input_tokens": "42770d1eedfa3fca",
        "hash_cont_tokens": "8688f339b9fbfe2c"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2364,
      "non_padded": 84,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:public_relations|0": {
      "hashes": {
        "hash_examples": "52a84bb75dd812eb",
        "hash_full_prompts": "52a84bb75dd812eb",
        "hash_input_tokens": "5fba1759550647a1",
        "hash_cont_tokens": "ba439106d4e0e8a5"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 398,
      "non_padded": 42,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:security_studies|0": {
      "hashes": {
        "hash_examples": "2469ceb06f350432",
        "hash_full_prompts": "2469ceb06f350432",
        "hash_input_tokens": "73e7fe139aa7ad05",
        "hash_cont_tokens": "76872d770f0588d0"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 944,
      "non_padded": 36,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:sociology|0": {
      "hashes": {
        "hash_examples": "d2060dedb3fc2bea",
        "hash_full_prompts": "d2060dedb3fc2bea",
        "hash_input_tokens": "016dea10212f649e",
        "hash_cont_tokens": "553c93e7469b2370"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 747,
      "non_padded": 57,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:us_foreign_policy|0": {
      "hashes": {
        "hash_examples": "84e882e740d43f01",
        "hash_full_prompts": "84e882e740d43f01",
        "hash_input_tokens": "cb0a28188e82e30c",
        "hash_cont_tokens": "f7dacbcf4c12192e"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 373,
      "non_padded": 27,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:virology|0": {
      "hashes": {
        "hash_examples": "0428d2d277aa56aa",
        "hash_full_prompts": "0428d2d277aa56aa",
        "hash_input_tokens": "c5da9860726a4e42",
        "hash_cont_tokens": "cf21f14e7122825c"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 614,
      "non_padded": 50,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:world_religions|0": {
      "hashes": {
        "hash_examples": "7e8f045c67ba6ba1",
        "hash_full_prompts": "7e8f045c67ba6ba1",
        "hash_input_tokens": "f132e6f15278f47c",
        "hash_cont_tokens": "ec9b2ef91888ec59"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 634,
      "non_padded": 50,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|openbookqa|0": {
      "hashes": {
        "hash_examples": "fd427af2ef0577e3",
        "hash_full_prompts": "fd427af2ef0577e3",
        "hash_input_tokens": "c0fdbda881575e0c",
        "hash_cont_tokens": "c7b680eb9c858d5f"
      },
      "truncated": 0,
      "non_truncated": 500,
      "padded": 1990,
      "non_padded": 10,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|piqa|0": {
      "hashes": {
        "hash_examples": "f7e288a8894cd149",
        "hash_full_prompts": "f7e288a8894cd149",
        "hash_input_tokens": "16f5fd94bc53d32e",
        "hash_cont_tokens": "c74853740fbd4159"
      },
      "truncated": 0,
      "non_truncated": 1838,
      "padded": 3554,
      "non_padded": 122,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|winogrande|0": {
      "hashes": {
        "hash_examples": "087d5d1a1afd4c7b",
        "hash_full_prompts": "087d5d1a1afd4c7b",
        "hash_input_tokens": "837d38312121c11a",
        "hash_cont_tokens": "07fdfa56b379c911"
      },
      "truncated": 0,
      "non_truncated": 1267,
      "padded": 2518,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|trivia_qa|0": {
      "hashes": {
        "hash_examples": "1e083041cb75ff0c",
        "hash_full_prompts": "1e083041cb75ff0c",
        "hash_input_tokens": "6d9bbd76baa542e2",
        "hash_cont_tokens": "6456373d71a9206b"
      },
      "truncated": 527,
      "non_truncated": 17417,
      "padded": 17944,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "8c5d115902bdf2aa",
      "hash_full_prompts": "8c5d115902bdf2aa",
      "hash_input_tokens": "7f18aa2cc552a80b",
      "hash_cont_tokens": "993467b107ef33a9"
    },
    "truncated": 527,
    "non_truncated": 48654,
    "padded": 134093,
    "non_padded": 2585,
    "num_truncated_few_shots": 0
  }
}