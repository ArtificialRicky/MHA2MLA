{
  "config_general": {
    "lighteval_sha": "?",
    "num_fewshot_seeds": 1,
    "override_batch_size": -1,
    "max_samples": null,
    "job_id": "",
    "start_time": 13300448.159326024,
    "end_time": 13301057.53744674,
    "total_evaluation_time_secondes": "609.3781207147986",
    "model_name": "_home_binguo_data_MLA-FT_checkpoints_lr_1e-4_cooldown_10_18000_hf",
    "model_sha": "",
    "model_dtype": null,
    "model_size": null
  },
  "results": {
    "custom|arc:challenge|0": {
      "acc": 0.24658703071672355,
      "acc_stderr": 0.012595726268790125,
      "acc_norm": 0.28668941979522183,
      "acc_norm_stderr": 0.013214986329274765
    },
    "custom|arc:easy|0": {
      "acc": 0.6106902356902357,
      "acc_stderr": 0.01000521278287814,
      "acc_norm": 0.5521885521885522,
      "acc_norm_stderr": 0.010203742451111527
    },
    "custom|hellaswag|0": {
      "acc": 0.34554869547898825,
      "acc_stderr": 0.004745749538752323,
      "acc_norm": 0.4108743278231428,
      "acc_norm_stderr": 0.004909870006388835
    },
    "custom|mmlu_cloze:abstract_algebra|0": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816508,
      "acc_norm": 0.19,
      "acc_norm_stderr": 0.03942772444036624
    },
    "custom|mmlu_cloze:anatomy|0": {
      "acc": 0.362962962962963,
      "acc_stderr": 0.04153948404742398,
      "acc_norm": 0.31851851851851853,
      "acc_norm_stderr": 0.04024778401977108
    },
    "custom|mmlu_cloze:astronomy|0": {
      "acc": 0.29605263157894735,
      "acc_stderr": 0.03715062154998904,
      "acc_norm": 0.39473684210526316,
      "acc_norm_stderr": 0.039777499346220734
    },
    "custom|mmlu_cloze:business_ethics|0": {
      "acc": 0.5,
      "acc_stderr": 0.050251890762960605,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "custom|mmlu_cloze:clinical_knowledge|0": {
      "acc": 0.26037735849056604,
      "acc_stderr": 0.027008766090708097,
      "acc_norm": 0.35471698113207545,
      "acc_norm_stderr": 0.029445175328199586
    },
    "custom|mmlu_cloze:college_biology|0": {
      "acc": 0.3402777777777778,
      "acc_stderr": 0.03962135573486219,
      "acc_norm": 0.3402777777777778,
      "acc_norm_stderr": 0.03962135573486219
    },
    "custom|mmlu_cloze:college_chemistry|0": {
      "acc": 0.22,
      "acc_stderr": 0.04163331998932269,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.0440844002276808
    },
    "custom|mmlu_cloze:college_computer_science|0": {
      "acc": 0.26,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "custom|mmlu_cloze:college_mathematics|0": {
      "acc": 0.16,
      "acc_stderr": 0.03684529491774709,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.04020151261036846
    },
    "custom|mmlu_cloze:college_medicine|0": {
      "acc": 0.2947976878612717,
      "acc_stderr": 0.034765996075164785,
      "acc_norm": 0.2658959537572254,
      "acc_norm_stderr": 0.033687629322594295
    },
    "custom|mmlu_cloze:college_physics|0": {
      "acc": 0.21568627450980393,
      "acc_stderr": 0.04092563958237655,
      "acc_norm": 0.17647058823529413,
      "acc_norm_stderr": 0.03793281185307809
    },
    "custom|mmlu_cloze:computer_security|0": {
      "acc": 0.29,
      "acc_stderr": 0.04560480215720684,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "custom|mmlu_cloze:conceptual_physics|0": {
      "acc": 0.3404255319148936,
      "acc_stderr": 0.030976692998534432,
      "acc_norm": 0.23404255319148937,
      "acc_norm_stderr": 0.02767845257821241
    },
    "custom|mmlu_cloze:econometrics|0": {
      "acc": 0.2543859649122807,
      "acc_stderr": 0.040969851398436716,
      "acc_norm": 0.22807017543859648,
      "acc_norm_stderr": 0.03947152782669415
    },
    "custom|mmlu_cloze:electrical_engineering|0": {
      "acc": 0.3103448275862069,
      "acc_stderr": 0.03855289616378949,
      "acc_norm": 0.32413793103448274,
      "acc_norm_stderr": 0.039004320691855554
    },
    "custom|mmlu_cloze:elementary_mathematics|0": {
      "acc": 0.23544973544973544,
      "acc_stderr": 0.02185150982203173,
      "acc_norm": 0.23544973544973544,
      "acc_norm_stderr": 0.021851509822031726
    },
    "custom|mmlu_cloze:formal_logic|0": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.04040610178208841,
      "acc_norm": 0.2698412698412698,
      "acc_norm_stderr": 0.03970158273235172
    },
    "custom|mmlu_cloze:global_facts|0": {
      "acc": 0.19,
      "acc_stderr": 0.039427724440366234,
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.040936018074033256
    },
    "custom|mmlu_cloze:high_school_biology|0": {
      "acc": 0.31290322580645163,
      "acc_stderr": 0.026377567028645854,
      "acc_norm": 0.34838709677419355,
      "acc_norm_stderr": 0.02710482632810094
    },
    "custom|mmlu_cloze:high_school_chemistry|0": {
      "acc": 0.20689655172413793,
      "acc_stderr": 0.02850137816789395,
      "acc_norm": 0.2561576354679803,
      "acc_norm_stderr": 0.0307127300709826
    },
    "custom|mmlu_cloze:high_school_computer_science|0": {
      "acc": 0.26,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720684
    },
    "custom|mmlu_cloze:high_school_european_history|0": {
      "acc": 0.28484848484848485,
      "acc_stderr": 0.035243908445117836,
      "acc_norm": 0.4484848484848485,
      "acc_norm_stderr": 0.038835659779569286
    },
    "custom|mmlu_cloze:high_school_geography|0": {
      "acc": 0.29292929292929293,
      "acc_stderr": 0.032424979581788166,
      "acc_norm": 0.3383838383838384,
      "acc_norm_stderr": 0.03371124142626303
    },
    "custom|mmlu_cloze:high_school_government_and_politics|0": {
      "acc": 0.36787564766839376,
      "acc_stderr": 0.03480175668466037,
      "acc_norm": 0.35751295336787564,
      "acc_norm_stderr": 0.034588160421810066
    },
    "custom|mmlu_cloze:high_school_macroeconomics|0": {
      "acc": 0.2717948717948718,
      "acc_stderr": 0.022556551010132375,
      "acc_norm": 0.2641025641025641,
      "acc_norm_stderr": 0.022352193737453285
    },
    "custom|mmlu_cloze:high_school_mathematics|0": {
      "acc": 0.14444444444444443,
      "acc_stderr": 0.021433761274104894,
      "acc_norm": 0.15925925925925927,
      "acc_norm_stderr": 0.022310394630040625
    },
    "custom|mmlu_cloze:high_school_microeconomics|0": {
      "acc": 0.2689075630252101,
      "acc_stderr": 0.02880139219363128,
      "acc_norm": 0.35294117647058826,
      "acc_norm_stderr": 0.031041941304059288
    },
    "custom|mmlu_cloze:high_school_physics|0": {
      "acc": 0.31125827814569534,
      "acc_stderr": 0.03780445850526732,
      "acc_norm": 0.2582781456953642,
      "acc_norm_stderr": 0.035737053147634576
    },
    "custom|mmlu_cloze:high_school_psychology|0": {
      "acc": 0.3926605504587156,
      "acc_stderr": 0.020937505161201096,
      "acc_norm": 0.363302752293578,
      "acc_norm_stderr": 0.020620603919625804
    },
    "custom|mmlu_cloze:high_school_statistics|0": {
      "acc": 0.2824074074074074,
      "acc_stderr": 0.030701372111510934,
      "acc_norm": 0.2824074074074074,
      "acc_norm_stderr": 0.030701372111510927
    },
    "custom|mmlu_cloze:high_school_us_history|0": {
      "acc": 0.25980392156862747,
      "acc_stderr": 0.030778554678693268,
      "acc_norm": 0.3480392156862745,
      "acc_norm_stderr": 0.03343311240488419
    },
    "custom|mmlu_cloze:high_school_world_history|0": {
      "acc": 0.27848101265822783,
      "acc_stderr": 0.029178682304842555,
      "acc_norm": 0.32489451476793246,
      "acc_norm_stderr": 0.03048603938910531
    },
    "custom|mmlu_cloze:human_aging|0": {
      "acc": 0.38565022421524664,
      "acc_stderr": 0.03266842214289201,
      "acc_norm": 0.33183856502242154,
      "acc_norm_stderr": 0.03160295143776679
    },
    "custom|mmlu_cloze:human_sexuality|0": {
      "acc": 0.40458015267175573,
      "acc_stderr": 0.043046937953806645,
      "acc_norm": 0.35877862595419846,
      "acc_norm_stderr": 0.04206739313864908
    },
    "custom|mmlu_cloze:international_law|0": {
      "acc": 0.1487603305785124,
      "acc_stderr": 0.032484700838071943,
      "acc_norm": 0.2809917355371901,
      "acc_norm_stderr": 0.04103203830514512
    },
    "custom|mmlu_cloze:jurisprudence|0": {
      "acc": 0.19444444444444445,
      "acc_stderr": 0.038260763248848646,
      "acc_norm": 0.3425925925925926,
      "acc_norm_stderr": 0.045879047413018105
    },
    "custom|mmlu_cloze:logical_fallacies|0": {
      "acc": 0.294478527607362,
      "acc_stderr": 0.03581165790474082,
      "acc_norm": 0.3067484662576687,
      "acc_norm_stderr": 0.03623089915724146
    },
    "custom|mmlu_cloze:machine_learning|0": {
      "acc": 0.25,
      "acc_stderr": 0.04109974682633932,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.04287858751340456
    },
    "custom|mmlu_cloze:management|0": {
      "acc": 0.3106796116504854,
      "acc_stderr": 0.04582124160161549,
      "acc_norm": 0.39805825242718446,
      "acc_norm_stderr": 0.0484674825397724
    },
    "custom|mmlu_cloze:marketing|0": {
      "acc": 0.44017094017094016,
      "acc_stderr": 0.0325207417206305,
      "acc_norm": 0.3803418803418803,
      "acc_norm_stderr": 0.03180425204384099
    },
    "custom|mmlu_cloze:medical_genetics|0": {
      "acc": 0.29,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "custom|mmlu_cloze:miscellaneous|0": {
      "acc": 0.3959131545338442,
      "acc_stderr": 0.017488247006979266,
      "acc_norm": 0.36015325670498083,
      "acc_norm_stderr": 0.01716636247136931
    },
    "custom|mmlu_cloze:moral_disputes|0": {
      "acc": 0.24855491329479767,
      "acc_stderr": 0.023267528432100174,
      "acc_norm": 0.2398843930635838,
      "acc_norm_stderr": 0.022989592543123567
    },
    "custom|mmlu_cloze:moral_scenarios|0": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.27262569832402234,
      "acc_norm_stderr": 0.014893391735249588
    },
    "custom|mmlu_cloze:nutrition|0": {
      "acc": 0.26143790849673204,
      "acc_stderr": 0.025160998214292456,
      "acc_norm": 0.3464052287581699,
      "acc_norm_stderr": 0.02724561304721535
    },
    "custom|mmlu_cloze:philosophy|0": {
      "acc": 0.26366559485530544,
      "acc_stderr": 0.02502553850053234,
      "acc_norm": 0.2540192926045016,
      "acc_norm_stderr": 0.024723861504771696
    },
    "custom|mmlu_cloze:prehistory|0": {
      "acc": 0.36728395061728397,
      "acc_stderr": 0.026822801759507898,
      "acc_norm": 0.2716049382716049,
      "acc_norm_stderr": 0.02474862449053738
    },
    "custom|mmlu_cloze:professional_accounting|0": {
      "acc": 0.24468085106382978,
      "acc_stderr": 0.025645553622266726,
      "acc_norm": 0.21631205673758866,
      "acc_norm_stderr": 0.02456172056056281
    },
    "custom|mmlu_cloze:professional_law|0": {
      "acc": 0.24445893089960888,
      "acc_stderr": 0.010976425013113897,
      "acc_norm": 0.26727509778357234,
      "acc_norm_stderr": 0.01130260751563752
    },
    "custom|mmlu_cloze:professional_medicine|0": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.026799562024887674,
      "acc_norm": 0.3125,
      "acc_norm_stderr": 0.02815637344037142
    },
    "custom|mmlu_cloze:professional_psychology|0": {
      "acc": 0.2973856209150327,
      "acc_stderr": 0.018492596536396955,
      "acc_norm": 0.28431372549019607,
      "acc_norm_stderr": 0.018249024411207664
    },
    "custom|mmlu_cloze:public_relations|0": {
      "acc": 0.37272727272727274,
      "acc_stderr": 0.046313813194254635,
      "acc_norm": 0.24545454545454545,
      "acc_norm_stderr": 0.04122066502878284
    },
    "custom|mmlu_cloze:security_studies|0": {
      "acc": 0.31020408163265306,
      "acc_stderr": 0.029613459872484378,
      "acc_norm": 0.24081632653061225,
      "acc_norm_stderr": 0.027372942201788167
    },
    "custom|mmlu_cloze:sociology|0": {
      "acc": 0.26865671641791045,
      "acc_stderr": 0.03134328358208954,
      "acc_norm": 0.2935323383084577,
      "acc_norm_stderr": 0.03220024104534204
    },
    "custom|mmlu_cloze:us_foreign_policy|0": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720683
    },
    "custom|mmlu_cloze:virology|0": {
      "acc": 0.26506024096385544,
      "acc_stderr": 0.03436024037944967,
      "acc_norm": 0.3313253012048193,
      "acc_norm_stderr": 0.03664314777288085
    },
    "custom|mmlu_cloze:world_religions|0": {
      "acc": 0.3684210526315789,
      "acc_stderr": 0.036996580176568775,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.03811079669833531
    },
    "custom|openbookqa|0": {
      "acc": 0.208,
      "acc_stderr": 0.018169542221229885,
      "acc_norm": 0.324,
      "acc_norm_stderr": 0.020950557312477445
    },
    "custom|piqa|0": {
      "acc": 0.676278563656148,
      "acc_stderr": 0.010916765010708773,
      "acc_norm": 0.6789989118607181,
      "acc_norm_stderr": 0.010892641574707906
    },
    "custom|winogrande|0": {
      "acc": 0.5090765588003157,
      "acc_stderr": 0.014050170094497707,
      "acc_norm": 0.5114443567482242,
      "acc_norm_stderr": 0.014048804199859332
    },
    "custom|trivia_qa|0": {
      "qem": 0.049264378065091395,
      "qem_stderr": 0.001615655847507646
    },
    "custom|arc:_average|0": {
      "acc": 0.4286386332034796,
      "acc_stderr": 0.011300469525834134,
      "acc_norm": 0.419438985991887,
      "acc_norm_stderr": 0.011709364390193145
    },
    "custom|mmlu_cloze:_average|0": {
      "acc": 0.2875718512420304,
      "acc_stderr": 0.03337775846226487,
      "acc_norm": 0.3005270312661747,
      "acc_norm_stderr": 0.0339536733985851
    },
    "all": {
      "acc": 0.3013932794466372,
      "acc_stderr": 0.03131770473438024,
      "acc_norm": 0.3157815293744098,
      "acc_norm_stderr": 0.031898095009415414,
      "qem": 0.049264378065091395,
      "qem_stderr": 0.001615655847507646
    }
  },
  "versions": {
    "custom|arc:challenge|0": 0,
    "custom|arc:easy|0": 0,
    "custom|hellaswag|0": 0,
    "custom|mmlu_cloze:abstract_algebra|0": 0,
    "custom|mmlu_cloze:anatomy|0": 0,
    "custom|mmlu_cloze:astronomy|0": 0,
    "custom|mmlu_cloze:business_ethics|0": 0,
    "custom|mmlu_cloze:clinical_knowledge|0": 0,
    "custom|mmlu_cloze:college_biology|0": 0,
    "custom|mmlu_cloze:college_chemistry|0": 0,
    "custom|mmlu_cloze:college_computer_science|0": 0,
    "custom|mmlu_cloze:college_mathematics|0": 0,
    "custom|mmlu_cloze:college_medicine|0": 0,
    "custom|mmlu_cloze:college_physics|0": 0,
    "custom|mmlu_cloze:computer_security|0": 0,
    "custom|mmlu_cloze:conceptual_physics|0": 0,
    "custom|mmlu_cloze:econometrics|0": 0,
    "custom|mmlu_cloze:electrical_engineering|0": 0,
    "custom|mmlu_cloze:elementary_mathematics|0": 0,
    "custom|mmlu_cloze:formal_logic|0": 0,
    "custom|mmlu_cloze:global_facts|0": 0,
    "custom|mmlu_cloze:high_school_biology|0": 0,
    "custom|mmlu_cloze:high_school_chemistry|0": 0,
    "custom|mmlu_cloze:high_school_computer_science|0": 0,
    "custom|mmlu_cloze:high_school_european_history|0": 0,
    "custom|mmlu_cloze:high_school_geography|0": 0,
    "custom|mmlu_cloze:high_school_government_and_politics|0": 0,
    "custom|mmlu_cloze:high_school_macroeconomics|0": 0,
    "custom|mmlu_cloze:high_school_mathematics|0": 0,
    "custom|mmlu_cloze:high_school_microeconomics|0": 0,
    "custom|mmlu_cloze:high_school_physics|0": 0,
    "custom|mmlu_cloze:high_school_psychology|0": 0,
    "custom|mmlu_cloze:high_school_statistics|0": 0,
    "custom|mmlu_cloze:high_school_us_history|0": 0,
    "custom|mmlu_cloze:high_school_world_history|0": 0,
    "custom|mmlu_cloze:human_aging|0": 0,
    "custom|mmlu_cloze:human_sexuality|0": 0,
    "custom|mmlu_cloze:international_law|0": 0,
    "custom|mmlu_cloze:jurisprudence|0": 0,
    "custom|mmlu_cloze:logical_fallacies|0": 0,
    "custom|mmlu_cloze:machine_learning|0": 0,
    "custom|mmlu_cloze:management|0": 0,
    "custom|mmlu_cloze:marketing|0": 0,
    "custom|mmlu_cloze:medical_genetics|0": 0,
    "custom|mmlu_cloze:miscellaneous|0": 0,
    "custom|mmlu_cloze:moral_disputes|0": 0,
    "custom|mmlu_cloze:moral_scenarios|0": 0,
    "custom|mmlu_cloze:nutrition|0": 0,
    "custom|mmlu_cloze:philosophy|0": 0,
    "custom|mmlu_cloze:prehistory|0": 0,
    "custom|mmlu_cloze:professional_accounting|0": 0,
    "custom|mmlu_cloze:professional_law|0": 0,
    "custom|mmlu_cloze:professional_medicine|0": 0,
    "custom|mmlu_cloze:professional_psychology|0": 0,
    "custom|mmlu_cloze:public_relations|0": 0,
    "custom|mmlu_cloze:security_studies|0": 0,
    "custom|mmlu_cloze:sociology|0": 0,
    "custom|mmlu_cloze:us_foreign_policy|0": 0,
    "custom|mmlu_cloze:virology|0": 0,
    "custom|mmlu_cloze:world_religions|0": 0,
    "custom|openbookqa|0": 0,
    "custom|piqa|0": 0,
    "custom|trivia_qa|0": 0,
    "custom|winogrande|0": 0
  },
  "config_tasks": {
    "custom|arc:challenge": {
      "name": "arc:challenge",
      "prompt_function": "arc",
      "hf_repo": "ai2_arc",
      "hf_subset": "ARC-Challenge",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1172,
      "effective_num_docs": 1172,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|arc:easy": {
      "name": "arc:easy",
      "prompt_function": "arc",
      "hf_repo": "ai2_arc",
      "hf_subset": "ARC-Easy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 2376,
      "effective_num_docs": 2376,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|hellaswag": {
      "name": "hellaswag",
      "prompt_function": "hellaswag_prompt",
      "hf_repo": "hellaswag",
      "hf_subset": "default",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 10042,
      "effective_num_docs": 10042,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:abstract_algebra": {
      "name": "mmlu_cloze:abstract_algebra",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "abstract_algebra",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:anatomy": {
      "name": "mmlu_cloze:anatomy",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "anatomy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:astronomy": {
      "name": "mmlu_cloze:astronomy",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "astronomy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:business_ethics": {
      "name": "mmlu_cloze:business_ethics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "business_ethics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:clinical_knowledge": {
      "name": "mmlu_cloze:clinical_knowledge",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "clinical_knowledge",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_biology": {
      "name": "mmlu_cloze:college_biology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_biology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_chemistry": {
      "name": "mmlu_cloze:college_chemistry",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_chemistry",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_computer_science": {
      "name": "mmlu_cloze:college_computer_science",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_computer_science",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_mathematics": {
      "name": "mmlu_cloze:college_mathematics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_mathematics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_medicine": {
      "name": "mmlu_cloze:college_medicine",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_medicine",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_physics": {
      "name": "mmlu_cloze:college_physics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_physics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:computer_security": {
      "name": "mmlu_cloze:computer_security",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "computer_security",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:conceptual_physics": {
      "name": "mmlu_cloze:conceptual_physics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "conceptual_physics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:econometrics": {
      "name": "mmlu_cloze:econometrics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "econometrics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:electrical_engineering": {
      "name": "mmlu_cloze:electrical_engineering",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "electrical_engineering",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:elementary_mathematics": {
      "name": "mmlu_cloze:elementary_mathematics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "elementary_mathematics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:formal_logic": {
      "name": "mmlu_cloze:formal_logic",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "formal_logic",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:global_facts": {
      "name": "mmlu_cloze:global_facts",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "global_facts",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_biology": {
      "name": "mmlu_cloze:high_school_biology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_biology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_chemistry": {
      "name": "mmlu_cloze:high_school_chemistry",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_chemistry",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_computer_science": {
      "name": "mmlu_cloze:high_school_computer_science",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_computer_science",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_european_history": {
      "name": "mmlu_cloze:high_school_european_history",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_european_history",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_geography": {
      "name": "mmlu_cloze:high_school_geography",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_geography",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_government_and_politics": {
      "name": "mmlu_cloze:high_school_government_and_politics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_macroeconomics": {
      "name": "mmlu_cloze:high_school_macroeconomics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_mathematics": {
      "name": "mmlu_cloze:high_school_mathematics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_mathematics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_microeconomics": {
      "name": "mmlu_cloze:high_school_microeconomics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_physics": {
      "name": "mmlu_cloze:high_school_physics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_physics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_psychology": {
      "name": "mmlu_cloze:high_school_psychology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_psychology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_statistics": {
      "name": "mmlu_cloze:high_school_statistics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_statistics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_us_history": {
      "name": "mmlu_cloze:high_school_us_history",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_us_history",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_world_history": {
      "name": "mmlu_cloze:high_school_world_history",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_world_history",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:human_aging": {
      "name": "mmlu_cloze:human_aging",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_aging",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:human_sexuality": {
      "name": "mmlu_cloze:human_sexuality",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_sexuality",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:international_law": {
      "name": "mmlu_cloze:international_law",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "international_law",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:jurisprudence": {
      "name": "mmlu_cloze:jurisprudence",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "jurisprudence",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:logical_fallacies": {
      "name": "mmlu_cloze:logical_fallacies",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "logical_fallacies",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:machine_learning": {
      "name": "mmlu_cloze:machine_learning",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "machine_learning",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:management": {
      "name": "mmlu_cloze:management",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "management",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:marketing": {
      "name": "mmlu_cloze:marketing",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "marketing",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:medical_genetics": {
      "name": "mmlu_cloze:medical_genetics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "medical_genetics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:miscellaneous": {
      "name": "mmlu_cloze:miscellaneous",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "miscellaneous",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:moral_disputes": {
      "name": "mmlu_cloze:moral_disputes",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_disputes",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:moral_scenarios": {
      "name": "mmlu_cloze:moral_scenarios",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_scenarios",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:nutrition": {
      "name": "mmlu_cloze:nutrition",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "nutrition",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:philosophy": {
      "name": "mmlu_cloze:philosophy",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "philosophy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:prehistory": {
      "name": "mmlu_cloze:prehistory",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "prehistory",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:professional_accounting": {
      "name": "mmlu_cloze:professional_accounting",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_accounting",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:professional_law": {
      "name": "mmlu_cloze:professional_law",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_law",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:professional_medicine": {
      "name": "mmlu_cloze:professional_medicine",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_medicine",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:professional_psychology": {
      "name": "mmlu_cloze:professional_psychology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_psychology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:public_relations": {
      "name": "mmlu_cloze:public_relations",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "public_relations",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:security_studies": {
      "name": "mmlu_cloze:security_studies",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "security_studies",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:sociology": {
      "name": "mmlu_cloze:sociology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "sociology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:us_foreign_policy": {
      "name": "mmlu_cloze:us_foreign_policy",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "us_foreign_policy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:virology": {
      "name": "mmlu_cloze:virology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "virology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:world_religions": {
      "name": "mmlu_cloze:world_religions",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "world_religions",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|openbookqa": {
      "name": "openbookqa",
      "prompt_function": "openbookqa",
      "hf_repo": "openbookqa",
      "hf_subset": "main",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 500,
      "effective_num_docs": 500,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|piqa": {
      "name": "piqa",
      "prompt_function": "piqa_harness",
      "hf_repo": "piqa",
      "hf_subset": "plain_text",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1838,
      "effective_num_docs": 1838,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|trivia_qa": {
      "name": "trivia_qa",
      "prompt_function": "triviaqa",
      "hf_repo": "mandarjoshi/trivia_qa",
      "hf_subset": "rc.nocontext",
      "metric": [
        {
          "metric_name": "qem",
          "higher_is_better": true,
          "category": "3",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation"
      ],
      "trust_dataset": true,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling_from_train",
      "generation_size": 20,
      "generation_grammar": null,
      "stop_sequence": [
        "\n",
        ".",
        ","
      ],
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 17944,
      "effective_num_docs": 17944,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|winogrande": {
      "name": "winogrande",
      "prompt_function": "winogrande",
      "hf_repo": "winogrande",
      "hf_subset": "winogrande_xl",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1267,
      "effective_num_docs": 1267,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    }
  },
  "summary_tasks": {
    "custom|arc:challenge|0": {
      "hashes": {
        "hash_examples": "17b0cae357c0259e",
        "hash_full_prompts": "17b0cae357c0259e",
        "hash_input_tokens": "c8c7df76162a72c4",
        "hash_cont_tokens": "8a6142410aa83459"
      },
      "truncated": 0,
      "non_truncated": 1172,
      "padded": 0,
      "non_padded": 4687,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|arc:easy|0": {
      "hashes": {
        "hash_examples": "63703c3cdff55bec",
        "hash_full_prompts": "63703c3cdff55bec",
        "hash_input_tokens": "4febe5b18b7d3725",
        "hash_cont_tokens": "05d2d5c4c5617983"
      },
      "truncated": 0,
      "non_truncated": 2376,
      "padded": 0,
      "non_padded": 9501,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|hellaswag|0": {
      "hashes": {
        "hash_examples": "31985c805c3a737e",
        "hash_full_prompts": "31985c805c3a737e",
        "hash_input_tokens": "36459e87226161e7",
        "hash_cont_tokens": "a24ad90286f5c45b"
      },
      "truncated": 0,
      "non_truncated": 10042,
      "padded": 0,
      "non_padded": 40168,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:abstract_algebra|0": {
      "hashes": {
        "hash_examples": "ff00c12a680621ba",
        "hash_full_prompts": "ff00c12a680621ba",
        "hash_input_tokens": "1c4c331f2628f617",
        "hash_cont_tokens": "98244a128feedaad"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 0,
      "non_padded": 400,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:anatomy|0": {
      "hashes": {
        "hash_examples": "7f9c9593991d6727",
        "hash_full_prompts": "7f9c9593991d6727",
        "hash_input_tokens": "896fb14ee8609f4b",
        "hash_cont_tokens": "4ac86efed43e1c51"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 0,
      "non_padded": 540,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:astronomy|0": {
      "hashes": {
        "hash_examples": "ff5985a306787836",
        "hash_full_prompts": "ff5985a306787836",
        "hash_input_tokens": "7409a8f095907e7f",
        "hash_cont_tokens": "e7f8d76eb393b75a"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 0,
      "non_padded": 608,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:business_ethics|0": {
      "hashes": {
        "hash_examples": "e3fe02a23d08c2d0",
        "hash_full_prompts": "e3fe02a23d08c2d0",
        "hash_input_tokens": "ce642b8c21cb37be",
        "hash_cont_tokens": "040dc345b170ffc5"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 0,
      "non_padded": 400,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:clinical_knowledge|0": {
      "hashes": {
        "hash_examples": "9b962be8e1615cd5",
        "hash_full_prompts": "9b962be8e1615cd5",
        "hash_input_tokens": "7b13a557d01d436e",
        "hash_cont_tokens": "e261baa054087060"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 0,
      "non_padded": 1060,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_biology|0": {
      "hashes": {
        "hash_examples": "17b4ca841de3a2a3",
        "hash_full_prompts": "17b4ca841de3a2a3",
        "hash_input_tokens": "eda8d8f67fb8dd7a",
        "hash_cont_tokens": "37d725554362ae9d"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 0,
      "non_padded": 576,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_chemistry|0": {
      "hashes": {
        "hash_examples": "babea49005fd8249",
        "hash_full_prompts": "babea49005fd8249",
        "hash_input_tokens": "e686750c6b7158bf",
        "hash_cont_tokens": "c889bd71046b1327"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 0,
      "non_padded": 400,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_computer_science|0": {
      "hashes": {
        "hash_examples": "47f4fef1846c8914",
        "hash_full_prompts": "47f4fef1846c8914",
        "hash_input_tokens": "e532ca16adb0dfc4",
        "hash_cont_tokens": "37dafcaa9aa2f52f"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 0,
      "non_padded": 400,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_mathematics|0": {
      "hashes": {
        "hash_examples": "4d3686d599963414",
        "hash_full_prompts": "4d3686d599963414",
        "hash_input_tokens": "c7972885ca18ad49",
        "hash_cont_tokens": "be3bb8b25af71c1a"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 0,
      "non_padded": 400,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_medicine|0": {
      "hashes": {
        "hash_examples": "7209619ecac6f235",
        "hash_full_prompts": "7209619ecac6f235",
        "hash_input_tokens": "98751527a386c25f",
        "hash_cont_tokens": "fcfceda2f6319181"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 0,
      "non_padded": 692,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_physics|0": {
      "hashes": {
        "hash_examples": "6131b6c60dd7f055",
        "hash_full_prompts": "6131b6c60dd7f055",
        "hash_input_tokens": "f6af7d0329d5a6f0",
        "hash_cont_tokens": "2ec73409e55174e6"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 0,
      "non_padded": 408,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:computer_security|0": {
      "hashes": {
        "hash_examples": "99539c9a5bc98a59",
        "hash_full_prompts": "99539c9a5bc98a59",
        "hash_input_tokens": "4607f676d72d9536",
        "hash_cont_tokens": "f777b1454a03f3ff"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 0,
      "non_padded": 400,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:conceptual_physics|0": {
      "hashes": {
        "hash_examples": "4e15015839d00858",
        "hash_full_prompts": "4e15015839d00858",
        "hash_input_tokens": "c1c1add82085f51c",
        "hash_cont_tokens": "08bd2929d3e8dae3"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 0,
      "non_padded": 940,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:econometrics|0": {
      "hashes": {
        "hash_examples": "fba5c647465e89e0",
        "hash_full_prompts": "fba5c647465e89e0",
        "hash_input_tokens": "ffb63ef0f7adcb3a",
        "hash_cont_tokens": "ef65564574d46829"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 0,
      "non_padded": 456,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:electrical_engineering|0": {
      "hashes": {
        "hash_examples": "2db2ba0cb98cda51",
        "hash_full_prompts": "2db2ba0cb98cda51",
        "hash_input_tokens": "f27118c7ed58365d",
        "hash_cont_tokens": "4ee41c8867286f43"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 0,
      "non_padded": 580,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:elementary_mathematics|0": {
      "hashes": {
        "hash_examples": "f231cd5ae05742bb",
        "hash_full_prompts": "f231cd5ae05742bb",
        "hash_input_tokens": "dee5a70793fb6e94",
        "hash_cont_tokens": "a62cf1187dfeaf53"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 0,
      "non_padded": 1512,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:formal_logic|0": {
      "hashes": {
        "hash_examples": "b69d9607d5da536e",
        "hash_full_prompts": "b69d9607d5da536e",
        "hash_input_tokens": "444ee9c33df3486a",
        "hash_cont_tokens": "341dea52e4ac56b3"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 0,
      "non_padded": 504,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:global_facts|0": {
      "hashes": {
        "hash_examples": "c9f53772e672f6bc",
        "hash_full_prompts": "c9f53772e672f6bc",
        "hash_input_tokens": "69b0e360c7d7cf14",
        "hash_cont_tokens": "dcb83a686271e34b"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 0,
      "non_padded": 400,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_biology|0": {
      "hashes": {
        "hash_examples": "455027cf6cdd02bc",
        "hash_full_prompts": "455027cf6cdd02bc",
        "hash_input_tokens": "8188ad4aebc64546",
        "hash_cont_tokens": "0c7a593ef5de0d31"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 0,
      "non_padded": 1240,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_chemistry|0": {
      "hashes": {
        "hash_examples": "95d9caac9edbc34d",
        "hash_full_prompts": "95d9caac9edbc34d",
        "hash_input_tokens": "9581d2b5488e31fb",
        "hash_cont_tokens": "0743e7a1a8ff2d0a"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 0,
      "non_padded": 812,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_computer_science|0": {
      "hashes": {
        "hash_examples": "6e44706db3791e51",
        "hash_full_prompts": "6e44706db3791e51",
        "hash_input_tokens": "ecc38cc59f0a92cf",
        "hash_cont_tokens": "09d9d2372305ffb8"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 0,
      "non_padded": 400,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_european_history|0": {
      "hashes": {
        "hash_examples": "9078fce41897117d",
        "hash_full_prompts": "9078fce41897117d",
        "hash_input_tokens": "206e036cd10d0b0d",
        "hash_cont_tokens": "0dd53f1d4dcd101e"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 0,
      "non_padded": 660,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_geography|0": {
      "hashes": {
        "hash_examples": "c68adcc34130a2e8",
        "hash_full_prompts": "c68adcc34130a2e8",
        "hash_input_tokens": "37808555b80d7e2f",
        "hash_cont_tokens": "f94d640f99fb4329"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 0,
      "non_padded": 792,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_government_and_politics|0": {
      "hashes": {
        "hash_examples": "6f839b19e49a0858",
        "hash_full_prompts": "6f839b19e49a0858",
        "hash_input_tokens": "1638891f09a7ea31",
        "hash_cont_tokens": "daba9a551e34db95"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 0,
      "non_padded": 772,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_macroeconomics|0": {
      "hashes": {
        "hash_examples": "708a5c05e7adb220",
        "hash_full_prompts": "708a5c05e7adb220",
        "hash_input_tokens": "6b5a969d53c8f3b9",
        "hash_cont_tokens": "ee0b967955f2a46d"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 0,
      "non_padded": 1560,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_mathematics|0": {
      "hashes": {
        "hash_examples": "47c621dd61fd7790",
        "hash_full_prompts": "47c621dd61fd7790",
        "hash_input_tokens": "6561dac1f23f0f6a",
        "hash_cont_tokens": "6380d87affb2b9f8"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 0,
      "non_padded": 1080,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_microeconomics|0": {
      "hashes": {
        "hash_examples": "3c467180d90f6371",
        "hash_full_prompts": "3c467180d90f6371",
        "hash_input_tokens": "17cc23a462897255",
        "hash_cont_tokens": "5df1d1d86fd88c6d"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 0,
      "non_padded": 952,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_physics|0": {
      "hashes": {
        "hash_examples": "89a598cdde43be79",
        "hash_full_prompts": "89a598cdde43be79",
        "hash_input_tokens": "b462cd060e6d82ec",
        "hash_cont_tokens": "de5436e283d4ee83"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 0,
      "non_padded": 604,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_psychology|0": {
      "hashes": {
        "hash_examples": "d8a7bf3f17ec12d0",
        "hash_full_prompts": "d8a7bf3f17ec12d0",
        "hash_input_tokens": "59a07f189c248a19",
        "hash_cont_tokens": "3631632fc1057eab"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 0,
      "non_padded": 2180,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_statistics|0": {
      "hashes": {
        "hash_examples": "995374a6caaa97d6",
        "hash_full_prompts": "995374a6caaa97d6",
        "hash_input_tokens": "2d7a56d4e01e1864",
        "hash_cont_tokens": "09f1141c7529d82a"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 0,
      "non_padded": 864,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_us_history|0": {
      "hashes": {
        "hash_examples": "7893e9d07e34cb37",
        "hash_full_prompts": "7893e9d07e34cb37",
        "hash_input_tokens": "22fe7e73e01168d3",
        "hash_cont_tokens": "8d9a597dd42902b3"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 0,
      "non_padded": 816,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_world_history|0": {
      "hashes": {
        "hash_examples": "48879684e37d1716",
        "hash_full_prompts": "48879684e37d1716",
        "hash_input_tokens": "d27d320be76a7e0f",
        "hash_cont_tokens": "4a0b117c3b7d2238"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 0,
      "non_padded": 948,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:human_aging|0": {
      "hashes": {
        "hash_examples": "afae8c53bd6e5f44",
        "hash_full_prompts": "afae8c53bd6e5f44",
        "hash_input_tokens": "1486230f592c7ef8",
        "hash_cont_tokens": "7d3e1ed9117fe082"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 0,
      "non_padded": 892,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:human_sexuality|0": {
      "hashes": {
        "hash_examples": "9701f02004912a7a",
        "hash_full_prompts": "9701f02004912a7a",
        "hash_input_tokens": "54e9c41d422a67dc",
        "hash_cont_tokens": "a27c963282147037"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 0,
      "non_padded": 524,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:international_law|0": {
      "hashes": {
        "hash_examples": "47955196de2d2c7a",
        "hash_full_prompts": "47955196de2d2c7a",
        "hash_input_tokens": "96d016e56cde72d7",
        "hash_cont_tokens": "3cdab75fc8da5caa"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 0,
      "non_padded": 484,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:jurisprudence|0": {
      "hashes": {
        "hash_examples": "a992eac2b8ae8bc4",
        "hash_full_prompts": "a992eac2b8ae8bc4",
        "hash_input_tokens": "7d0b88ad7747e779",
        "hash_cont_tokens": "cc005cbcae4008d9"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 0,
      "non_padded": 432,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:logical_fallacies|0": {
      "hashes": {
        "hash_examples": "b0d31ed08f699e6c",
        "hash_full_prompts": "b0d31ed08f699e6c",
        "hash_input_tokens": "758981784f46074f",
        "hash_cont_tokens": "995235329a5b51f5"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 0,
      "non_padded": 652,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:machine_learning|0": {
      "hashes": {
        "hash_examples": "dccdef2bae4461a6",
        "hash_full_prompts": "dccdef2bae4461a6",
        "hash_input_tokens": "23b902b259a0be07",
        "hash_cont_tokens": "f0e505798de38816"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 0,
      "non_padded": 448,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:management|0": {
      "hashes": {
        "hash_examples": "f600be25303e1fe2",
        "hash_full_prompts": "f600be25303e1fe2",
        "hash_input_tokens": "36e9911cd60c3918",
        "hash_cont_tokens": "c955c9ed4535b5d7"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 0,
      "non_padded": 412,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:marketing|0": {
      "hashes": {
        "hash_examples": "1a0df3ae5e306669",
        "hash_full_prompts": "1a0df3ae5e306669",
        "hash_input_tokens": "840b2087a60d8a5f",
        "hash_cont_tokens": "ac09441c1c1f3433"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 0,
      "non_padded": 936,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:medical_genetics|0": {
      "hashes": {
        "hash_examples": "eb87c9cfd9b7c760",
        "hash_full_prompts": "eb87c9cfd9b7c760",
        "hash_input_tokens": "9a6a0eab911ad751",
        "hash_cont_tokens": "6bcffabfb92f5b76"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 0,
      "non_padded": 400,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:miscellaneous|0": {
      "hashes": {
        "hash_examples": "f88d724036ba03b7",
        "hash_full_prompts": "f88d724036ba03b7",
        "hash_input_tokens": "11c38e1314409975",
        "hash_cont_tokens": "e4d006136da90a46"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 0,
      "non_padded": 3132,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:moral_disputes|0": {
      "hashes": {
        "hash_examples": "79782c0823005e7b",
        "hash_full_prompts": "79782c0823005e7b",
        "hash_input_tokens": "730bb81b8cef4860",
        "hash_cont_tokens": "c425bd965c05e2b4"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 0,
      "non_padded": 1384,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:moral_scenarios|0": {
      "hashes": {
        "hash_examples": "fe57fdd86442b483",
        "hash_full_prompts": "fe57fdd86442b483",
        "hash_input_tokens": "d1d6277a4543cf08",
        "hash_cont_tokens": "227bbd4433bb0b90"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 0,
      "non_padded": 3580,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:nutrition|0": {
      "hashes": {
        "hash_examples": "421f206f5957e90f",
        "hash_full_prompts": "421f206f5957e90f",
        "hash_input_tokens": "2b39a888bc199e16",
        "hash_cont_tokens": "d30d408beb445530"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 0,
      "non_padded": 1224,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:philosophy|0": {
      "hashes": {
        "hash_examples": "c93073e383957fc4",
        "hash_full_prompts": "c93073e383957fc4",
        "hash_input_tokens": "44bb3015115c503e",
        "hash_cont_tokens": "da894224d1bda1dc"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 0,
      "non_padded": 1244,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:prehistory|0": {
      "hashes": {
        "hash_examples": "58ec03e20eae9f90",
        "hash_full_prompts": "58ec03e20eae9f90",
        "hash_input_tokens": "47b715dcdea36e81",
        "hash_cont_tokens": "86812ff92274f71d"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 0,
      "non_padded": 1296,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:professional_accounting|0": {
      "hashes": {
        "hash_examples": "13f7a6023a118512",
        "hash_full_prompts": "13f7a6023a118512",
        "hash_input_tokens": "3874941977405d18",
        "hash_cont_tokens": "e658c06b69f613e3"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 0,
      "non_padded": 1128,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:professional_law|0": {
      "hashes": {
        "hash_examples": "8086d24f4d4e82f4",
        "hash_full_prompts": "8086d24f4d4e82f4",
        "hash_input_tokens": "fe2d47ceff2461a2",
        "hash_cont_tokens": "ffa99fd14aa09fc7"
      },
      "truncated": 0,
      "non_truncated": 1534,
      "padded": 0,
      "non_padded": 6136,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:professional_medicine|0": {
      "hashes": {
        "hash_examples": "f0b30b4e786eaeea",
        "hash_full_prompts": "f0b30b4e786eaeea",
        "hash_input_tokens": "7196ae52f0b08bc1",
        "hash_cont_tokens": "1c2f9fed2c254c36"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 0,
      "non_padded": 1088,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:professional_psychology|0": {
      "hashes": {
        "hash_examples": "29fd2b4a194c28ea",
        "hash_full_prompts": "29fd2b4a194c28ea",
        "hash_input_tokens": "baeaddcb354e72b9",
        "hash_cont_tokens": "3b35f927a7f96fb9"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 0,
      "non_padded": 2448,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:public_relations|0": {
      "hashes": {
        "hash_examples": "52a84bb75dd812eb",
        "hash_full_prompts": "52a84bb75dd812eb",
        "hash_input_tokens": "84568e4f9679df58",
        "hash_cont_tokens": "f3457e4eabdbe88c"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 0,
      "non_padded": 440,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:security_studies|0": {
      "hashes": {
        "hash_examples": "2469ceb06f350432",
        "hash_full_prompts": "2469ceb06f350432",
        "hash_input_tokens": "563714337e04995d",
        "hash_cont_tokens": "637d41c09fbe4d24"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 0,
      "non_padded": 980,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:sociology|0": {
      "hashes": {
        "hash_examples": "d2060dedb3fc2bea",
        "hash_full_prompts": "d2060dedb3fc2bea",
        "hash_input_tokens": "134f48dcdfcb7c99",
        "hash_cont_tokens": "67116c3366d67da5"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 0,
      "non_padded": 804,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:us_foreign_policy|0": {
      "hashes": {
        "hash_examples": "84e882e740d43f01",
        "hash_full_prompts": "84e882e740d43f01",
        "hash_input_tokens": "c749e7f75c8b4710",
        "hash_cont_tokens": "9497952e61ab6c26"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 0,
      "non_padded": 400,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:virology|0": {
      "hashes": {
        "hash_examples": "0428d2d277aa56aa",
        "hash_full_prompts": "0428d2d277aa56aa",
        "hash_input_tokens": "9a0908cf8958ab17",
        "hash_cont_tokens": "e51690deb9875de5"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 0,
      "non_padded": 664,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:world_religions|0": {
      "hashes": {
        "hash_examples": "7e8f045c67ba6ba1",
        "hash_full_prompts": "7e8f045c67ba6ba1",
        "hash_input_tokens": "f6ee0b90ebb3a374",
        "hash_cont_tokens": "3e4869fedb372b63"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 0,
      "non_padded": 684,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|openbookqa|0": {
      "hashes": {
        "hash_examples": "fd427af2ef0577e3",
        "hash_full_prompts": "fd427af2ef0577e3",
        "hash_input_tokens": "9beb75a721555437",
        "hash_cont_tokens": "f630bfe40f1358a4"
      },
      "truncated": 0,
      "non_truncated": 500,
      "padded": 0,
      "non_padded": 2000,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|piqa|0": {
      "hashes": {
        "hash_examples": "f7e288a8894cd149",
        "hash_full_prompts": "f7e288a8894cd149",
        "hash_input_tokens": "e91c8c6a4220848a",
        "hash_cont_tokens": "c8b40c82da4b7bcf"
      },
      "truncated": 0,
      "non_truncated": 1838,
      "padded": 0,
      "non_padded": 3676,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|winogrande|0": {
      "hashes": {
        "hash_examples": "087d5d1a1afd4c7b",
        "hash_full_prompts": "087d5d1a1afd4c7b",
        "hash_input_tokens": "13e09e09008d8b4f",
        "hash_cont_tokens": "a582506195155894"
      },
      "truncated": 0,
      "non_truncated": 1267,
      "padded": 0,
      "non_padded": 2534,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|trivia_qa|0": {
      "hashes": {
        "hash_examples": "1e083041cb75ff0c",
        "hash_full_prompts": "1e083041cb75ff0c",
        "hash_input_tokens": "3743c7e4df0522a6",
        "hash_cont_tokens": "5da41b6089a56ba0"
      },
      "truncated": 0,
      "non_truncated": 17944,
      "padded": 0,
      "non_padded": 17944,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "8c5d115902bdf2aa",
      "hash_full_prompts": "8c5d115902bdf2aa",
      "hash_input_tokens": "4466a81dfd3dc309",
      "hash_cont_tokens": "829db1dccfcff56f"
    },
    "truncated": 0,
    "non_truncated": 49181,
    "padded": 0,
    "non_padded": 136678,
    "num_truncated_few_shots": 0
  }
}