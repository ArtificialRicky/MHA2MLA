{
  "config_general": {
    "lighteval_sha": "?",
    "num_fewshot_seeds": 1,
    "override_batch_size": 96,
    "max_samples": null,
    "job_id": "",
    "start_time": 171703.795271745,
    "end_time": 172365.102600855,
    "total_evaluation_time_secondes": "661.3073291099863",
    "model_name": ".._checkpoints_v2_start0_step4_rope_9000_hf",
    "model_sha": "",
    "model_dtype": "torch.bfloat16",
    "model_size": "256.57 MB"
  },
  "results": {
    "custom|arc:challenge|0": {
      "acc": 0.25,
      "acc_stderr": 0.012653835621466646,
      "acc_norm": 0.28071672354948807,
      "acc_norm_stderr": 0.013131238126975578
    },
    "custom|arc:easy|0": {
      "acc": 0.593013468013468,
      "acc_stderr": 0.010080695355466601,
      "acc_norm": 0.523989898989899,
      "acc_norm_stderr": 0.010247967392742682
    },
    "custom|hellaswag|0": {
      "acc": 0.3406691894045011,
      "acc_stderr": 0.004729656826803946,
      "acc_norm": 0.40240987851025695,
      "acc_norm_stderr": 0.004893814890208319
    },
    "custom|mmlu_cloze:abstract_algebra|0": {
      "acc": 0.19,
      "acc_stderr": 0.039427724440366234,
      "acc_norm": 0.17,
      "acc_norm_stderr": 0.03775251680686371
    },
    "custom|mmlu_cloze:anatomy|0": {
      "acc": 0.3111111111111111,
      "acc_stderr": 0.0399926287661772,
      "acc_norm": 0.31851851851851853,
      "acc_norm_stderr": 0.04024778401977108
    },
    "custom|mmlu_cloze:astronomy|0": {
      "acc": 0.28289473684210525,
      "acc_stderr": 0.03665349695640767,
      "acc_norm": 0.3881578947368421,
      "acc_norm_stderr": 0.03965842097512744
    },
    "custom|mmlu_cloze:business_ethics|0": {
      "acc": 0.5,
      "acc_stderr": 0.050251890762960605,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "custom|mmlu_cloze:clinical_knowledge|0": {
      "acc": 0.23773584905660378,
      "acc_stderr": 0.026199808807561915,
      "acc_norm": 0.33584905660377357,
      "acc_norm_stderr": 0.029067220146644826
    },
    "custom|mmlu_cloze:college_biology|0": {
      "acc": 0.3194444444444444,
      "acc_stderr": 0.03899073687357336,
      "acc_norm": 0.3402777777777778,
      "acc_norm_stderr": 0.03962135573486219
    },
    "custom|mmlu_cloze:college_chemistry|0": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.0440844002276808
    },
    "custom|mmlu_cloze:college_computer_science|0": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542129,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "custom|mmlu_cloze:college_mathematics|0": {
      "acc": 0.15,
      "acc_stderr": 0.03588702812826371,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.04163331998932269
    },
    "custom|mmlu_cloze:college_medicine|0": {
      "acc": 0.2658959537572254,
      "acc_stderr": 0.03368762932259431,
      "acc_norm": 0.24855491329479767,
      "acc_norm_stderr": 0.03295304696818318
    },
    "custom|mmlu_cloze:college_physics|0": {
      "acc": 0.17647058823529413,
      "acc_stderr": 0.0379328118530781,
      "acc_norm": 0.16666666666666666,
      "acc_norm_stderr": 0.03708284662416545
    },
    "custom|mmlu_cloze:computer_security|0": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768077,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110196
    },
    "custom|mmlu_cloze:conceptual_physics|0": {
      "acc": 0.32340425531914896,
      "acc_stderr": 0.030579442773610337,
      "acc_norm": 0.225531914893617,
      "acc_norm_stderr": 0.02732107841738753
    },
    "custom|mmlu_cloze:econometrics|0": {
      "acc": 0.23684210526315788,
      "acc_stderr": 0.039994238792813344,
      "acc_norm": 0.22807017543859648,
      "acc_norm_stderr": 0.03947152782669415
    },
    "custom|mmlu_cloze:electrical_engineering|0": {
      "acc": 0.2827586206896552,
      "acc_stderr": 0.03752833958003337,
      "acc_norm": 0.31724137931034485,
      "acc_norm_stderr": 0.03878352372138622
    },
    "custom|mmlu_cloze:elementary_mathematics|0": {
      "acc": 0.25132275132275134,
      "acc_stderr": 0.022340482339643895,
      "acc_norm": 0.25132275132275134,
      "acc_norm_stderr": 0.022340482339643895
    },
    "custom|mmlu_cloze:formal_logic|0": {
      "acc": 0.2619047619047619,
      "acc_stderr": 0.039325376803928724,
      "acc_norm": 0.2698412698412698,
      "acc_norm_stderr": 0.03970158273235173
    },
    "custom|mmlu_cloze:global_facts|0": {
      "acc": 0.24,
      "acc_stderr": 0.042923469599092816,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "custom|mmlu_cloze:high_school_biology|0": {
      "acc": 0.3064516129032258,
      "acc_stderr": 0.026226485652553883,
      "acc_norm": 0.34516129032258064,
      "acc_norm_stderr": 0.027045746573534327
    },
    "custom|mmlu_cloze:high_school_chemistry|0": {
      "acc": 0.2019704433497537,
      "acc_stderr": 0.02824735012218028,
      "acc_norm": 0.23645320197044334,
      "acc_norm_stderr": 0.029896114291733552
    },
    "custom|mmlu_cloze:high_school_computer_science|0": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909282,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621503
    },
    "custom|mmlu_cloze:high_school_european_history|0": {
      "acc": 0.28484848484848485,
      "acc_stderr": 0.03524390844511784,
      "acc_norm": 0.4484848484848485,
      "acc_norm_stderr": 0.038835659779569286
    },
    "custom|mmlu_cloze:high_school_geography|0": {
      "acc": 0.2676767676767677,
      "acc_stderr": 0.031544498882702866,
      "acc_norm": 0.3383838383838384,
      "acc_norm_stderr": 0.03371124142626303
    },
    "custom|mmlu_cloze:high_school_government_and_politics|0": {
      "acc": 0.35751295336787564,
      "acc_stderr": 0.034588160421810066,
      "acc_norm": 0.34196891191709844,
      "acc_norm_stderr": 0.034234651001042844
    },
    "custom|mmlu_cloze:high_school_macroeconomics|0": {
      "acc": 0.2564102564102564,
      "acc_stderr": 0.022139081103971534,
      "acc_norm": 0.2717948717948718,
      "acc_norm_stderr": 0.022556551010132368
    },
    "custom|mmlu_cloze:high_school_mathematics|0": {
      "acc": 0.13333333333333333,
      "acc_stderr": 0.020726180448133864,
      "acc_norm": 0.12962962962962962,
      "acc_norm_stderr": 0.020479910253320705
    },
    "custom|mmlu_cloze:high_school_microeconomics|0": {
      "acc": 0.2773109243697479,
      "acc_stderr": 0.029079374539480007,
      "acc_norm": 0.36134453781512604,
      "acc_norm_stderr": 0.03120469122515002
    },
    "custom|mmlu_cloze:high_school_physics|0": {
      "acc": 0.2980132450331126,
      "acc_stderr": 0.037345356767871984,
      "acc_norm": 0.2582781456953642,
      "acc_norm_stderr": 0.035737053147634576
    },
    "custom|mmlu_cloze:high_school_psychology|0": {
      "acc": 0.3761467889908257,
      "acc_stderr": 0.020769231968205074,
      "acc_norm": 0.3614678899082569,
      "acc_norm_stderr": 0.02059808200993736
    },
    "custom|mmlu_cloze:high_school_statistics|0": {
      "acc": 0.2916666666666667,
      "acc_stderr": 0.030998666304560545,
      "acc_norm": 0.2916666666666667,
      "acc_norm_stderr": 0.030998666304560534
    },
    "custom|mmlu_cloze:high_school_us_history|0": {
      "acc": 0.29411764705882354,
      "acc_stderr": 0.0319800166011507,
      "acc_norm": 0.3431372549019608,
      "acc_norm_stderr": 0.03332139944668085
    },
    "custom|mmlu_cloze:high_school_world_history|0": {
      "acc": 0.28270042194092826,
      "acc_stderr": 0.029312814153955914,
      "acc_norm": 0.3037974683544304,
      "acc_norm_stderr": 0.029936696387138605
    },
    "custom|mmlu_cloze:human_aging|0": {
      "acc": 0.38565022421524664,
      "acc_stderr": 0.03266842214289201,
      "acc_norm": 0.35874439461883406,
      "acc_norm_stderr": 0.03219079200419996
    },
    "custom|mmlu_cloze:human_sexuality|0": {
      "acc": 0.4198473282442748,
      "acc_stderr": 0.04328577215262971,
      "acc_norm": 0.35877862595419846,
      "acc_norm_stderr": 0.04206739313864908
    },
    "custom|mmlu_cloze:international_law|0": {
      "acc": 0.14049586776859505,
      "acc_stderr": 0.031722334260021606,
      "acc_norm": 0.2809917355371901,
      "acc_norm_stderr": 0.04103203830514512
    },
    "custom|mmlu_cloze:jurisprudence|0": {
      "acc": 0.19444444444444445,
      "acc_stderr": 0.038260763248848646,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04557239513497752
    },
    "custom|mmlu_cloze:logical_fallacies|0": {
      "acc": 0.2822085889570552,
      "acc_stderr": 0.03536117886664743,
      "acc_norm": 0.3128834355828221,
      "acc_norm_stderr": 0.036429145782924055
    },
    "custom|mmlu_cloze:machine_learning|0": {
      "acc": 0.26785714285714285,
      "acc_stderr": 0.04203277291467763,
      "acc_norm": 0.26785714285714285,
      "acc_norm_stderr": 0.04203277291467763
    },
    "custom|mmlu_cloze:management|0": {
      "acc": 0.3106796116504854,
      "acc_stderr": 0.04582124160161549,
      "acc_norm": 0.39805825242718446,
      "acc_norm_stderr": 0.0484674825397724
    },
    "custom|mmlu_cloze:marketing|0": {
      "acc": 0.38461538461538464,
      "acc_stderr": 0.03187195347942466,
      "acc_norm": 0.37606837606837606,
      "acc_norm_stderr": 0.03173393632969482
    },
    "custom|mmlu_cloze:medical_genetics|0": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "custom|mmlu_cloze:miscellaneous|0": {
      "acc": 0.37420178799489145,
      "acc_stderr": 0.017304805072252034,
      "acc_norm": 0.3486590038314176,
      "acc_norm_stderr": 0.017041243143490946
    },
    "custom|mmlu_cloze:moral_disputes|0": {
      "acc": 0.23121387283236994,
      "acc_stderr": 0.022698657167855713,
      "acc_norm": 0.24277456647398843,
      "acc_norm_stderr": 0.0230836585869842
    },
    "custom|mmlu_cloze:moral_scenarios|0": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.27262569832402234,
      "acc_norm_stderr": 0.014893391735249588
    },
    "custom|mmlu_cloze:nutrition|0": {
      "acc": 0.26143790849673204,
      "acc_stderr": 0.025160998214292456,
      "acc_norm": 0.34967320261437906,
      "acc_norm_stderr": 0.0273053080762747
    },
    "custom|mmlu_cloze:philosophy|0": {
      "acc": 0.2315112540192926,
      "acc_stderr": 0.023956532766639133,
      "acc_norm": 0.2765273311897106,
      "acc_norm_stderr": 0.02540383297817962
    },
    "custom|mmlu_cloze:prehistory|0": {
      "acc": 0.3395061728395062,
      "acc_stderr": 0.026348564412011635,
      "acc_norm": 0.25617283950617287,
      "acc_norm_stderr": 0.024288533637726095
    },
    "custom|mmlu_cloze:professional_accounting|0": {
      "acc": 0.23049645390070922,
      "acc_stderr": 0.025123739226872405,
      "acc_norm": 0.2127659574468085,
      "acc_norm_stderr": 0.024414612974307706
    },
    "custom|mmlu_cloze:professional_law|0": {
      "acc": 0.24185136897001303,
      "acc_stderr": 0.010936550813827061,
      "acc_norm": 0.27509778357235987,
      "acc_norm_stderr": 0.01140544362099694
    },
    "custom|mmlu_cloze:professional_medicine|0": {
      "acc": 0.23897058823529413,
      "acc_stderr": 0.025905280644893006,
      "acc_norm": 0.2867647058823529,
      "acc_norm_stderr": 0.02747227447323382
    },
    "custom|mmlu_cloze:professional_psychology|0": {
      "acc": 0.28431372549019607,
      "acc_stderr": 0.01824902441120767,
      "acc_norm": 0.2908496732026144,
      "acc_norm_stderr": 0.018373116915903966
    },
    "custom|mmlu_cloze:public_relations|0": {
      "acc": 0.38181818181818183,
      "acc_stderr": 0.04653429807913509,
      "acc_norm": 0.2636363636363636,
      "acc_norm_stderr": 0.04220224692971987
    },
    "custom|mmlu_cloze:security_studies|0": {
      "acc": 0.3020408163265306,
      "acc_stderr": 0.029393609319879815,
      "acc_norm": 0.22448979591836735,
      "acc_norm_stderr": 0.02671143055553841
    },
    "custom|mmlu_cloze:sociology|0": {
      "acc": 0.2736318407960199,
      "acc_stderr": 0.031524391865554044,
      "acc_norm": 0.2835820895522388,
      "acc_norm_stderr": 0.031871875379197986
    },
    "custom|mmlu_cloze:us_foreign_policy|0": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768079,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720683
    },
    "custom|mmlu_cloze:virology|0": {
      "acc": 0.25903614457831325,
      "acc_stderr": 0.034106466140718564,
      "acc_norm": 0.3373493975903614,
      "acc_norm_stderr": 0.0368078369072758
    },
    "custom|mmlu_cloze:world_religions|0": {
      "acc": 0.3567251461988304,
      "acc_stderr": 0.03674013002860954,
      "acc_norm": 0.4502923976608187,
      "acc_norm_stderr": 0.03815827365913237
    },
    "custom|openbookqa|0": {
      "acc": 0.226,
      "acc_stderr": 0.01872295644913993,
      "acc_norm": 0.324,
      "acc_norm_stderr": 0.020950557312477445
    },
    "custom|piqa|0": {
      "acc": 0.6713819368879217,
      "acc_stderr": 0.010959127105167044,
      "acc_norm": 0.6735582154515778,
      "acc_norm_stderr": 0.010940467046177302
    },
    "custom|winogrande|0": {
      "acc": 0.5272296764009471,
      "acc_stderr": 0.0140316316298277,
      "acc_norm": 0.5090765588003157,
      "acc_norm_stderr": 0.01405017009449771
    },
    "custom|trivia_qa|0": {
      "qem": 0.02390771288452965,
      "qem_stderr": 0.0011404255214674578
    },
    "custom|arc:_average|0": {
      "acc": 0.421506734006734,
      "acc_stderr": 0.011367265488466623,
      "acc_norm": 0.40235331126969354,
      "acc_norm_stderr": 0.01168960275985913
    },
    "custom|mmlu_cloze:_average|0": {
      "acc": 0.27856995449054767,
      "acc_stderr": 0.033069486610330955,
      "acc_norm": 0.30016801714089697,
      "acc_norm_stderr": 0.03394391681924365
    },
    "all": {
      "acc": 0.29344097899473104,
      "acc_stderr": 0.031049819678995817,
      "acc_norm": 0.31465600400528043,
      "acc_norm_stderr": 0.03188916624698361,
      "qem": 0.02390771288452965,
      "qem_stderr": 0.0011404255214674578
    }
  },
  "versions": {
    "custom|arc:challenge|0": 0,
    "custom|arc:easy|0": 0,
    "custom|hellaswag|0": 0,
    "custom|mmlu_cloze:abstract_algebra|0": 0,
    "custom|mmlu_cloze:anatomy|0": 0,
    "custom|mmlu_cloze:astronomy|0": 0,
    "custom|mmlu_cloze:business_ethics|0": 0,
    "custom|mmlu_cloze:clinical_knowledge|0": 0,
    "custom|mmlu_cloze:college_biology|0": 0,
    "custom|mmlu_cloze:college_chemistry|0": 0,
    "custom|mmlu_cloze:college_computer_science|0": 0,
    "custom|mmlu_cloze:college_mathematics|0": 0,
    "custom|mmlu_cloze:college_medicine|0": 0,
    "custom|mmlu_cloze:college_physics|0": 0,
    "custom|mmlu_cloze:computer_security|0": 0,
    "custom|mmlu_cloze:conceptual_physics|0": 0,
    "custom|mmlu_cloze:econometrics|0": 0,
    "custom|mmlu_cloze:electrical_engineering|0": 0,
    "custom|mmlu_cloze:elementary_mathematics|0": 0,
    "custom|mmlu_cloze:formal_logic|0": 0,
    "custom|mmlu_cloze:global_facts|0": 0,
    "custom|mmlu_cloze:high_school_biology|0": 0,
    "custom|mmlu_cloze:high_school_chemistry|0": 0,
    "custom|mmlu_cloze:high_school_computer_science|0": 0,
    "custom|mmlu_cloze:high_school_european_history|0": 0,
    "custom|mmlu_cloze:high_school_geography|0": 0,
    "custom|mmlu_cloze:high_school_government_and_politics|0": 0,
    "custom|mmlu_cloze:high_school_macroeconomics|0": 0,
    "custom|mmlu_cloze:high_school_mathematics|0": 0,
    "custom|mmlu_cloze:high_school_microeconomics|0": 0,
    "custom|mmlu_cloze:high_school_physics|0": 0,
    "custom|mmlu_cloze:high_school_psychology|0": 0,
    "custom|mmlu_cloze:high_school_statistics|0": 0,
    "custom|mmlu_cloze:high_school_us_history|0": 0,
    "custom|mmlu_cloze:high_school_world_history|0": 0,
    "custom|mmlu_cloze:human_aging|0": 0,
    "custom|mmlu_cloze:human_sexuality|0": 0,
    "custom|mmlu_cloze:international_law|0": 0,
    "custom|mmlu_cloze:jurisprudence|0": 0,
    "custom|mmlu_cloze:logical_fallacies|0": 0,
    "custom|mmlu_cloze:machine_learning|0": 0,
    "custom|mmlu_cloze:management|0": 0,
    "custom|mmlu_cloze:marketing|0": 0,
    "custom|mmlu_cloze:medical_genetics|0": 0,
    "custom|mmlu_cloze:miscellaneous|0": 0,
    "custom|mmlu_cloze:moral_disputes|0": 0,
    "custom|mmlu_cloze:moral_scenarios|0": 0,
    "custom|mmlu_cloze:nutrition|0": 0,
    "custom|mmlu_cloze:philosophy|0": 0,
    "custom|mmlu_cloze:prehistory|0": 0,
    "custom|mmlu_cloze:professional_accounting|0": 0,
    "custom|mmlu_cloze:professional_law|0": 0,
    "custom|mmlu_cloze:professional_medicine|0": 0,
    "custom|mmlu_cloze:professional_psychology|0": 0,
    "custom|mmlu_cloze:public_relations|0": 0,
    "custom|mmlu_cloze:security_studies|0": 0,
    "custom|mmlu_cloze:sociology|0": 0,
    "custom|mmlu_cloze:us_foreign_policy|0": 0,
    "custom|mmlu_cloze:virology|0": 0,
    "custom|mmlu_cloze:world_religions|0": 0,
    "custom|openbookqa|0": 0,
    "custom|piqa|0": 0,
    "custom|trivia_qa|0": 0,
    "custom|winogrande|0": 0
  },
  "config_tasks": {
    "custom|arc:challenge": {
      "name": "arc:challenge",
      "prompt_function": "arc",
      "hf_repo": "ai2_arc",
      "hf_subset": "ARC-Challenge",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1172,
      "effective_num_docs": 1172,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|arc:easy": {
      "name": "arc:easy",
      "prompt_function": "arc",
      "hf_repo": "ai2_arc",
      "hf_subset": "ARC-Easy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 2376,
      "effective_num_docs": 2376,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|hellaswag": {
      "name": "hellaswag",
      "prompt_function": "hellaswag_prompt",
      "hf_repo": "hellaswag",
      "hf_subset": "default",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 10042,
      "effective_num_docs": 10042,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:abstract_algebra": {
      "name": "mmlu_cloze:abstract_algebra",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "abstract_algebra",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:anatomy": {
      "name": "mmlu_cloze:anatomy",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "anatomy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:astronomy": {
      "name": "mmlu_cloze:astronomy",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "astronomy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:business_ethics": {
      "name": "mmlu_cloze:business_ethics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "business_ethics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:clinical_knowledge": {
      "name": "mmlu_cloze:clinical_knowledge",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "clinical_knowledge",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_biology": {
      "name": "mmlu_cloze:college_biology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_biology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_chemistry": {
      "name": "mmlu_cloze:college_chemistry",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_chemistry",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_computer_science": {
      "name": "mmlu_cloze:college_computer_science",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_computer_science",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_mathematics": {
      "name": "mmlu_cloze:college_mathematics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_mathematics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_medicine": {
      "name": "mmlu_cloze:college_medicine",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_medicine",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:college_physics": {
      "name": "mmlu_cloze:college_physics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_physics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:computer_security": {
      "name": "mmlu_cloze:computer_security",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "computer_security",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:conceptual_physics": {
      "name": "mmlu_cloze:conceptual_physics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "conceptual_physics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:econometrics": {
      "name": "mmlu_cloze:econometrics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "econometrics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:electrical_engineering": {
      "name": "mmlu_cloze:electrical_engineering",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "electrical_engineering",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:elementary_mathematics": {
      "name": "mmlu_cloze:elementary_mathematics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "elementary_mathematics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:formal_logic": {
      "name": "mmlu_cloze:formal_logic",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "formal_logic",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:global_facts": {
      "name": "mmlu_cloze:global_facts",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "global_facts",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_biology": {
      "name": "mmlu_cloze:high_school_biology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_biology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_chemistry": {
      "name": "mmlu_cloze:high_school_chemistry",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_chemistry",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_computer_science": {
      "name": "mmlu_cloze:high_school_computer_science",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_computer_science",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_european_history": {
      "name": "mmlu_cloze:high_school_european_history",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_european_history",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_geography": {
      "name": "mmlu_cloze:high_school_geography",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_geography",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_government_and_politics": {
      "name": "mmlu_cloze:high_school_government_and_politics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_macroeconomics": {
      "name": "mmlu_cloze:high_school_macroeconomics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_mathematics": {
      "name": "mmlu_cloze:high_school_mathematics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_mathematics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_microeconomics": {
      "name": "mmlu_cloze:high_school_microeconomics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_physics": {
      "name": "mmlu_cloze:high_school_physics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_physics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_psychology": {
      "name": "mmlu_cloze:high_school_psychology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_psychology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_statistics": {
      "name": "mmlu_cloze:high_school_statistics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_statistics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_us_history": {
      "name": "mmlu_cloze:high_school_us_history",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_us_history",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:high_school_world_history": {
      "name": "mmlu_cloze:high_school_world_history",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_world_history",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:human_aging": {
      "name": "mmlu_cloze:human_aging",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_aging",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:human_sexuality": {
      "name": "mmlu_cloze:human_sexuality",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_sexuality",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:international_law": {
      "name": "mmlu_cloze:international_law",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "international_law",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:jurisprudence": {
      "name": "mmlu_cloze:jurisprudence",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "jurisprudence",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:logical_fallacies": {
      "name": "mmlu_cloze:logical_fallacies",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "logical_fallacies",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:machine_learning": {
      "name": "mmlu_cloze:machine_learning",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "machine_learning",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:management": {
      "name": "mmlu_cloze:management",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "management",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:marketing": {
      "name": "mmlu_cloze:marketing",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "marketing",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:medical_genetics": {
      "name": "mmlu_cloze:medical_genetics",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "medical_genetics",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:miscellaneous": {
      "name": "mmlu_cloze:miscellaneous",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "miscellaneous",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:moral_disputes": {
      "name": "mmlu_cloze:moral_disputes",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_disputes",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:moral_scenarios": {
      "name": "mmlu_cloze:moral_scenarios",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_scenarios",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:nutrition": {
      "name": "mmlu_cloze:nutrition",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "nutrition",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:philosophy": {
      "name": "mmlu_cloze:philosophy",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "philosophy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:prehistory": {
      "name": "mmlu_cloze:prehistory",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "prehistory",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:professional_accounting": {
      "name": "mmlu_cloze:professional_accounting",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_accounting",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:professional_law": {
      "name": "mmlu_cloze:professional_law",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_law",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:professional_medicine": {
      "name": "mmlu_cloze:professional_medicine",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_medicine",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:professional_psychology": {
      "name": "mmlu_cloze:professional_psychology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_psychology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:public_relations": {
      "name": "mmlu_cloze:public_relations",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "public_relations",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:security_studies": {
      "name": "mmlu_cloze:security_studies",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "security_studies",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:sociology": {
      "name": "mmlu_cloze:sociology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "sociology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:us_foreign_policy": {
      "name": "mmlu_cloze:us_foreign_policy",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "us_foreign_policy",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:virology": {
      "name": "mmlu_cloze:virology",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "virology",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|mmlu_cloze:world_religions": {
      "name": "mmlu_cloze:world_religions",
      "prompt_function": "mmlu_cloze_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "world_religions",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": null,
      "trust_dataset": false,
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|openbookqa": {
      "name": "openbookqa",
      "prompt_function": "openbookqa",
      "hf_repo": "openbookqa",
      "hf_subset": "main",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 500,
      "effective_num_docs": 500,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|piqa": {
      "name": "piqa",
      "prompt_function": "piqa_harness",
      "hf_repo": "piqa",
      "hf_subset": "plain_text",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1838,
      "effective_num_docs": 1838,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|trivia_qa": {
      "name": "trivia_qa",
      "prompt_function": "triviaqa",
      "hf_repo": "mandarjoshi/trivia_qa",
      "hf_subset": "rc.nocontext",
      "metric": [
        {
          "metric_name": "qem",
          "higher_is_better": true,
          "category": "3",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation"
      ],
      "trust_dataset": true,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling_from_train",
      "generation_size": 20,
      "generation_grammar": null,
      "stop_sequence": [
        "\n",
        ".",
        ","
      ],
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 17944,
      "effective_num_docs": 17944,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    },
    "custom|winogrande": {
      "name": "winogrande",
      "prompt_function": "winogrande",
      "hf_repo": "winogrande",
      "hf_subset": "winogrande_xl",
      "metric": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        },
        {
          "metric_name": "acc_norm",
          "higher_is_better": true,
          "category": "8",
          "use_case": "1",
          "sample_level_fn": "compute",
          "corpus_level_fn": "mean"
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "trust_dataset": false,
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "generation_grammar": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1267,
      "effective_num_docs": 1267,
      "must_remove_duplicate_docs": false,
      "version": 0,
      "frozen": false
    }
  },
  "summary_tasks": {
    "custom|arc:challenge|0": {
      "hashes": {
        "hash_examples": "17b0cae357c0259e",
        "hash_full_prompts": "17b0cae357c0259e",
        "hash_input_tokens": "ad17dbd81a2a3d26",
        "hash_cont_tokens": "c512920b4065b2e5"
      },
      "truncated": 0,
      "non_truncated": 1172,
      "padded": 4687,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|arc:easy|0": {
      "hashes": {
        "hash_examples": "63703c3cdff55bec",
        "hash_full_prompts": "63703c3cdff55bec",
        "hash_input_tokens": "40eb9e5845d99163",
        "hash_cont_tokens": "9819b2bd4e06b652"
      },
      "truncated": 0,
      "non_truncated": 2376,
      "padded": 9501,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|hellaswag|0": {
      "hashes": {
        "hash_examples": "31985c805c3a737e",
        "hash_full_prompts": "31985c805c3a737e",
        "hash_input_tokens": "6ed1e652f6c51d42",
        "hash_cont_tokens": "228934e77282dbf3"
      },
      "truncated": 0,
      "non_truncated": 10042,
      "padded": 39678,
      "non_padded": 490,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:abstract_algebra|0": {
      "hashes": {
        "hash_examples": "ff00c12a680621ba",
        "hash_full_prompts": "ff00c12a680621ba",
        "hash_input_tokens": "3deb93f0a8b89a12",
        "hash_cont_tokens": "3b1fd217ca5eae7d"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 394,
      "non_padded": 6,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:anatomy|0": {
      "hashes": {
        "hash_examples": "7f9c9593991d6727",
        "hash_full_prompts": "7f9c9593991d6727",
        "hash_input_tokens": "83eb2604b438a0ac",
        "hash_cont_tokens": "da5637ac437db1cd"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 506,
      "non_padded": 34,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:astronomy|0": {
      "hashes": {
        "hash_examples": "ff5985a306787836",
        "hash_full_prompts": "ff5985a306787836",
        "hash_input_tokens": "665132a1dc5de70c",
        "hash_cont_tokens": "5c634041250c2777"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 593,
      "non_padded": 15,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:business_ethics|0": {
      "hashes": {
        "hash_examples": "e3fe02a23d08c2d0",
        "hash_full_prompts": "e3fe02a23d08c2d0",
        "hash_input_tokens": "8b4c3ffaa76df3bf",
        "hash_cont_tokens": "53c23c096d6fbbc5"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 389,
      "non_padded": 11,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:clinical_knowledge|0": {
      "hashes": {
        "hash_examples": "9b962be8e1615cd5",
        "hash_full_prompts": "9b962be8e1615cd5",
        "hash_input_tokens": "e23904e043567e98",
        "hash_cont_tokens": "f0cbafe3545dd281"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 989,
      "non_padded": 71,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_biology|0": {
      "hashes": {
        "hash_examples": "17b4ca841de3a2a3",
        "hash_full_prompts": "17b4ca841de3a2a3",
        "hash_input_tokens": "ceb01e0604c75ada",
        "hash_cont_tokens": "bd120945b8cd6191"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 559,
      "non_padded": 17,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_chemistry|0": {
      "hashes": {
        "hash_examples": "babea49005fd8249",
        "hash_full_prompts": "babea49005fd8249",
        "hash_input_tokens": "df472866ea2a8a3d",
        "hash_cont_tokens": "01560744da3bb1ae"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 380,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_computer_science|0": {
      "hashes": {
        "hash_examples": "47f4fef1846c8914",
        "hash_full_prompts": "47f4fef1846c8914",
        "hash_input_tokens": "485962986df7fbb7",
        "hash_cont_tokens": "0e6deead73387308"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 386,
      "non_padded": 14,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_mathematics|0": {
      "hashes": {
        "hash_examples": "4d3686d599963414",
        "hash_full_prompts": "4d3686d599963414",
        "hash_input_tokens": "2e62aed19b952d56",
        "hash_cont_tokens": "8dafab7e4fbb7fbe"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 391,
      "non_padded": 9,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_medicine|0": {
      "hashes": {
        "hash_examples": "7209619ecac6f235",
        "hash_full_prompts": "7209619ecac6f235",
        "hash_input_tokens": "537efb793ce96ddc",
        "hash_cont_tokens": "84342b2b76c98a6e"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 669,
      "non_padded": 23,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:college_physics|0": {
      "hashes": {
        "hash_examples": "6131b6c60dd7f055",
        "hash_full_prompts": "6131b6c60dd7f055",
        "hash_input_tokens": "43de489cb2fb254d",
        "hash_cont_tokens": "9d0e49ba8dc5989a"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 399,
      "non_padded": 9,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:computer_security|0": {
      "hashes": {
        "hash_examples": "99539c9a5bc98a59",
        "hash_full_prompts": "99539c9a5bc98a59",
        "hash_input_tokens": "9e4ddd2397d35aa3",
        "hash_cont_tokens": "01121a08afaaf8dc"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 382,
      "non_padded": 18,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:conceptual_physics|0": {
      "hashes": {
        "hash_examples": "4e15015839d00858",
        "hash_full_prompts": "4e15015839d00858",
        "hash_input_tokens": "bf0f2e7060a7147a",
        "hash_cont_tokens": "e513df10450acb97"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 891,
      "non_padded": 49,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:econometrics|0": {
      "hashes": {
        "hash_examples": "fba5c647465e89e0",
        "hash_full_prompts": "fba5c647465e89e0",
        "hash_input_tokens": "0172a75cff9a807b",
        "hash_cont_tokens": "134b345bdee1a0cb"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 451,
      "non_padded": 5,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:electrical_engineering|0": {
      "hashes": {
        "hash_examples": "2db2ba0cb98cda51",
        "hash_full_prompts": "2db2ba0cb98cda51",
        "hash_input_tokens": "cb93841652392319",
        "hash_cont_tokens": "a683d95cb9fbf0d6"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 556,
      "non_padded": 24,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:elementary_mathematics|0": {
      "hashes": {
        "hash_examples": "f231cd5ae05742bb",
        "hash_full_prompts": "f231cd5ae05742bb",
        "hash_input_tokens": "2c622fca8451ea44",
        "hash_cont_tokens": "094ff4c560f0db79"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1448,
      "non_padded": 64,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:formal_logic|0": {
      "hashes": {
        "hash_examples": "b69d9607d5da536e",
        "hash_full_prompts": "b69d9607d5da536e",
        "hash_input_tokens": "3f9f9808ee4d5c08",
        "hash_cont_tokens": "29db55d952acb85d"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 499,
      "non_padded": 5,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:global_facts|0": {
      "hashes": {
        "hash_examples": "c9f53772e672f6bc",
        "hash_full_prompts": "c9f53772e672f6bc",
        "hash_input_tokens": "ce153de1bd9d7d1d",
        "hash_cont_tokens": "867dcd15c271e15c"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 394,
      "non_padded": 6,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_biology|0": {
      "hashes": {
        "hash_examples": "455027cf6cdd02bc",
        "hash_full_prompts": "455027cf6cdd02bc",
        "hash_input_tokens": "8ac752c32df9f4f5",
        "hash_cont_tokens": "cb3e6a68f47a8eeb"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1208,
      "non_padded": 32,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_chemistry|0": {
      "hashes": {
        "hash_examples": "95d9caac9edbc34d",
        "hash_full_prompts": "95d9caac9edbc34d",
        "hash_input_tokens": "df11c251b9b0d163",
        "hash_cont_tokens": "896d06c20dcec9ec"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 790,
      "non_padded": 22,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_computer_science|0": {
      "hashes": {
        "hash_examples": "6e44706db3791e51",
        "hash_full_prompts": "6e44706db3791e51",
        "hash_input_tokens": "bdeea952887d8dd7",
        "hash_cont_tokens": "9da53a8484fa853e"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 394,
      "non_padded": 6,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_european_history|0": {
      "hashes": {
        "hash_examples": "9078fce41897117d",
        "hash_full_prompts": "9078fce41897117d",
        "hash_input_tokens": "9ad11e4de3ce0c67",
        "hash_cont_tokens": "62e27833dc0027fe"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 660,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_geography|0": {
      "hashes": {
        "hash_examples": "c68adcc34130a2e8",
        "hash_full_prompts": "c68adcc34130a2e8",
        "hash_input_tokens": "4c9748f8e241a30d",
        "hash_cont_tokens": "0dde98155d0c26e6"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 752,
      "non_padded": 40,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_government_and_politics|0": {
      "hashes": {
        "hash_examples": "6f839b19e49a0858",
        "hash_full_prompts": "6f839b19e49a0858",
        "hash_input_tokens": "da410e087149f6bc",
        "hash_cont_tokens": "d74a61eae0b6b657"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 758,
      "non_padded": 14,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_macroeconomics|0": {
      "hashes": {
        "hash_examples": "708a5c05e7adb220",
        "hash_full_prompts": "708a5c05e7adb220",
        "hash_input_tokens": "70302329195e342b",
        "hash_cont_tokens": "6441e265e8f4af19"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1477,
      "non_padded": 83,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_mathematics|0": {
      "hashes": {
        "hash_examples": "47c621dd61fd7790",
        "hash_full_prompts": "47c621dd61fd7790",
        "hash_input_tokens": "c87690d64f3747c0",
        "hash_cont_tokens": "089e3cb73a2e8d5a"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1057,
      "non_padded": 23,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_microeconomics|0": {
      "hashes": {
        "hash_examples": "3c467180d90f6371",
        "hash_full_prompts": "3c467180d90f6371",
        "hash_input_tokens": "19e3ca96d87edb6b",
        "hash_cont_tokens": "b796174aba15bfad"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 895,
      "non_padded": 57,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_physics|0": {
      "hashes": {
        "hash_examples": "89a598cdde43be79",
        "hash_full_prompts": "89a598cdde43be79",
        "hash_input_tokens": "639b1ef057172e8c",
        "hash_cont_tokens": "842e2a4539ef99ab"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 597,
      "non_padded": 7,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_psychology|0": {
      "hashes": {
        "hash_examples": "d8a7bf3f17ec12d0",
        "hash_full_prompts": "d8a7bf3f17ec12d0",
        "hash_input_tokens": "80a9a600eb9eebaa",
        "hash_cont_tokens": "a77f669bd77c6c4d"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2100,
      "non_padded": 80,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_statistics|0": {
      "hashes": {
        "hash_examples": "995374a6caaa97d6",
        "hash_full_prompts": "995374a6caaa97d6",
        "hash_input_tokens": "2beb42f49ef9dd1b",
        "hash_cont_tokens": "7d7c20e92bc841a9"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 845,
      "non_padded": 19,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_us_history|0": {
      "hashes": {
        "hash_examples": "7893e9d07e34cb37",
        "hash_full_prompts": "7893e9d07e34cb37",
        "hash_input_tokens": "83a6ca8cc3ca2519",
        "hash_cont_tokens": "e117d838a8f12367"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 816,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:high_school_world_history|0": {
      "hashes": {
        "hash_examples": "48879684e37d1716",
        "hash_full_prompts": "48879684e37d1716",
        "hash_input_tokens": "48c38505c96e1601",
        "hash_cont_tokens": "5ba8b8b1bd31492f"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:human_aging|0": {
      "hashes": {
        "hash_examples": "afae8c53bd6e5f44",
        "hash_full_prompts": "afae8c53bd6e5f44",
        "hash_input_tokens": "b622d2ae2a9a2389",
        "hash_cont_tokens": "3ed6eb737b8d673d"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 843,
      "non_padded": 49,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:human_sexuality|0": {
      "hashes": {
        "hash_examples": "9701f02004912a7a",
        "hash_full_prompts": "9701f02004912a7a",
        "hash_input_tokens": "0590bed70f247fc8",
        "hash_cont_tokens": "a0ca80e5fea94af0"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 504,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:international_law|0": {
      "hashes": {
        "hash_examples": "47955196de2d2c7a",
        "hash_full_prompts": "47955196de2d2c7a",
        "hash_input_tokens": "0835116ea29b754c",
        "hash_cont_tokens": "e294ba24e42cee98"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 463,
      "non_padded": 21,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:jurisprudence|0": {
      "hashes": {
        "hash_examples": "a992eac2b8ae8bc4",
        "hash_full_prompts": "a992eac2b8ae8bc4",
        "hash_input_tokens": "1030710a6d14196a",
        "hash_cont_tokens": "0609d6898d9b68ad"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 419,
      "non_padded": 13,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:logical_fallacies|0": {
      "hashes": {
        "hash_examples": "b0d31ed08f699e6c",
        "hash_full_prompts": "b0d31ed08f699e6c",
        "hash_input_tokens": "d25d432910b49efe",
        "hash_cont_tokens": "185350eb12cb00c8"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 638,
      "non_padded": 14,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:machine_learning|0": {
      "hashes": {
        "hash_examples": "dccdef2bae4461a6",
        "hash_full_prompts": "dccdef2bae4461a6",
        "hash_input_tokens": "07a5e2893c11a61b",
        "hash_cont_tokens": "4246808b9997c875"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 441,
      "non_padded": 7,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:management|0": {
      "hashes": {
        "hash_examples": "f600be25303e1fe2",
        "hash_full_prompts": "f600be25303e1fe2",
        "hash_input_tokens": "10ff2ab1debb80e2",
        "hash_cont_tokens": "14f65210b4927437"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 392,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:marketing|0": {
      "hashes": {
        "hash_examples": "1a0df3ae5e306669",
        "hash_full_prompts": "1a0df3ae5e306669",
        "hash_input_tokens": "dd8fb6f3fc9075b9",
        "hash_cont_tokens": "52968c44575b2158"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 909,
      "non_padded": 27,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:medical_genetics|0": {
      "hashes": {
        "hash_examples": "eb87c9cfd9b7c760",
        "hash_full_prompts": "eb87c9cfd9b7c760",
        "hash_input_tokens": "e34c7451d5600add",
        "hash_cont_tokens": "11ee1d1bad20ade8"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 375,
      "non_padded": 25,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:miscellaneous|0": {
      "hashes": {
        "hash_examples": "f88d724036ba03b7",
        "hash_full_prompts": "f88d724036ba03b7",
        "hash_input_tokens": "2c64d670544bcb79",
        "hash_cont_tokens": "d84457206b083219"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 2994,
      "non_padded": 138,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:moral_disputes|0": {
      "hashes": {
        "hash_examples": "79782c0823005e7b",
        "hash_full_prompts": "79782c0823005e7b",
        "hash_input_tokens": "1ba722114b2f8fa8",
        "hash_cont_tokens": "85c70be841230602"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1341,
      "non_padded": 43,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:moral_scenarios|0": {
      "hashes": {
        "hash_examples": "fe57fdd86442b483",
        "hash_full_prompts": "fe57fdd86442b483",
        "hash_input_tokens": "d24c1c4d9202eaff",
        "hash_cont_tokens": "afb7f4b4b6fba335"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3376,
      "non_padded": 204,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:nutrition|0": {
      "hashes": {
        "hash_examples": "421f206f5957e90f",
        "hash_full_prompts": "421f206f5957e90f",
        "hash_input_tokens": "7231de6e2da3e3a5",
        "hash_cont_tokens": "09243c7215a5c8ff"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1151,
      "non_padded": 73,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:philosophy|0": {
      "hashes": {
        "hash_examples": "c93073e383957fc4",
        "hash_full_prompts": "c93073e383957fc4",
        "hash_input_tokens": "9ccba858995c9a71",
        "hash_cont_tokens": "0391eac7daac6e51"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1161,
      "non_padded": 83,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:prehistory|0": {
      "hashes": {
        "hash_examples": "58ec03e20eae9f90",
        "hash_full_prompts": "58ec03e20eae9f90",
        "hash_input_tokens": "2124f665ee7adbcd",
        "hash_cont_tokens": "f8d95d6e63c30de1"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1246,
      "non_padded": 50,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:professional_accounting|0": {
      "hashes": {
        "hash_examples": "13f7a6023a118512",
        "hash_full_prompts": "13f7a6023a118512",
        "hash_input_tokens": "fc4166484adb1c07",
        "hash_cont_tokens": "47a3d8a4fa184090"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1118,
      "non_padded": 10,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:professional_law|0": {
      "hashes": {
        "hash_examples": "8086d24f4d4e82f4",
        "hash_full_prompts": "8086d24f4d4e82f4",
        "hash_input_tokens": "39cad63797669b26",
        "hash_cont_tokens": "477ac5f3a2582801"
      },
      "truncated": 0,
      "non_truncated": 1534,
      "padded": 6119,
      "non_padded": 17,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:professional_medicine|0": {
      "hashes": {
        "hash_examples": "f0b30b4e786eaeea",
        "hash_full_prompts": "f0b30b4e786eaeea",
        "hash_input_tokens": "709cda008dac9ac1",
        "hash_cont_tokens": "9d55d18fcd916be3"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1084,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:professional_psychology|0": {
      "hashes": {
        "hash_examples": "29fd2b4a194c28ea",
        "hash_full_prompts": "29fd2b4a194c28ea",
        "hash_input_tokens": "42770d1eedfa3fca",
        "hash_cont_tokens": "8688f339b9fbfe2c"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2364,
      "non_padded": 84,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:public_relations|0": {
      "hashes": {
        "hash_examples": "52a84bb75dd812eb",
        "hash_full_prompts": "52a84bb75dd812eb",
        "hash_input_tokens": "5fba1759550647a1",
        "hash_cont_tokens": "ba439106d4e0e8a5"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 398,
      "non_padded": 42,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:security_studies|0": {
      "hashes": {
        "hash_examples": "2469ceb06f350432",
        "hash_full_prompts": "2469ceb06f350432",
        "hash_input_tokens": "73e7fe139aa7ad05",
        "hash_cont_tokens": "76872d770f0588d0"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 944,
      "non_padded": 36,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:sociology|0": {
      "hashes": {
        "hash_examples": "d2060dedb3fc2bea",
        "hash_full_prompts": "d2060dedb3fc2bea",
        "hash_input_tokens": "016dea10212f649e",
        "hash_cont_tokens": "553c93e7469b2370"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 747,
      "non_padded": 57,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:us_foreign_policy|0": {
      "hashes": {
        "hash_examples": "84e882e740d43f01",
        "hash_full_prompts": "84e882e740d43f01",
        "hash_input_tokens": "cb0a28188e82e30c",
        "hash_cont_tokens": "f7dacbcf4c12192e"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 373,
      "non_padded": 27,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:virology|0": {
      "hashes": {
        "hash_examples": "0428d2d277aa56aa",
        "hash_full_prompts": "0428d2d277aa56aa",
        "hash_input_tokens": "c5da9860726a4e42",
        "hash_cont_tokens": "cf21f14e7122825c"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 614,
      "non_padded": 50,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu_cloze:world_religions|0": {
      "hashes": {
        "hash_examples": "7e8f045c67ba6ba1",
        "hash_full_prompts": "7e8f045c67ba6ba1",
        "hash_input_tokens": "f132e6f15278f47c",
        "hash_cont_tokens": "ec9b2ef91888ec59"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 634,
      "non_padded": 50,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|openbookqa|0": {
      "hashes": {
        "hash_examples": "fd427af2ef0577e3",
        "hash_full_prompts": "fd427af2ef0577e3",
        "hash_input_tokens": "c0fdbda881575e0c",
        "hash_cont_tokens": "c7b680eb9c858d5f"
      },
      "truncated": 0,
      "non_truncated": 500,
      "padded": 1990,
      "non_padded": 10,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|piqa|0": {
      "hashes": {
        "hash_examples": "f7e288a8894cd149",
        "hash_full_prompts": "f7e288a8894cd149",
        "hash_input_tokens": "16f5fd94bc53d32e",
        "hash_cont_tokens": "c74853740fbd4159"
      },
      "truncated": 0,
      "non_truncated": 1838,
      "padded": 3554,
      "non_padded": 122,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|winogrande|0": {
      "hashes": {
        "hash_examples": "087d5d1a1afd4c7b",
        "hash_full_prompts": "087d5d1a1afd4c7b",
        "hash_input_tokens": "837d38312121c11a",
        "hash_cont_tokens": "07fdfa56b379c911"
      },
      "truncated": 0,
      "non_truncated": 1267,
      "padded": 2518,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|trivia_qa|0": {
      "hashes": {
        "hash_examples": "1e083041cb75ff0c",
        "hash_full_prompts": "1e083041cb75ff0c",
        "hash_input_tokens": "6d9bbd76baa542e2",
        "hash_cont_tokens": "6608e6fb4f2b0512"
      },
      "truncated": 527,
      "non_truncated": 17417,
      "padded": 17944,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "8c5d115902bdf2aa",
      "hash_full_prompts": "8c5d115902bdf2aa",
      "hash_input_tokens": "7f18aa2cc552a80b",
      "hash_cont_tokens": "cdf68b0f7ae9d355"
    },
    "truncated": 527,
    "non_truncated": 48654,
    "padded": 134093,
    "non_padded": 2585,
    "num_truncated_few_shots": 0
  }
}